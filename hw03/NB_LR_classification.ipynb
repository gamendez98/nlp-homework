{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7274554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:27:18.768796Z",
     "start_time": "2024-09-14T17:27:18.762695Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty_n_regex = r'''\n",
    "^From:.*?\\n|                      # Ignora la línea que empieza con 'From:' y lo que sigue hasta el final de la línea\n",
    "^Subject:(?:\\s*Re:\\s*)?(.*)\\n|    # Captura el texto del 'Subject' y elimina 'Subject:' o 'Re:'\n",
    ".*Date:.*?\\n|                      # Ignora la línea que empieza con 'Date:'\n",
    "^Archive-name:.*?\\n|              # Ignora la línea que empieza con 'Archive-name:' y lo que sigue hasta el final de la línea\n",
    "^Alt-atheism-archive-name:.*?\\n|  # Ignora la línea que empieza con 'Alt-atheism-archive-name:' y lo que sigue hasta el final de la línea\n",
    "^Last-modified:.*?\\n|             # Ignora la línea que empieza con 'Last-modified:' y lo que sigue hasta el final de la línea\n",
    "^Version:.*?\\n|                   # Ignora la línea que empieza con 'Version:' y lo que sigue hasta el final de la línea\n",
    "^.*@.*?\\n|                        # Ignora la línea que contiene '@' y lo que sigue hasta el final de la línea\n",
    "In\\sarticle.*?writes:\\n|          # Ignora todo lo que está entre 'In article...' y 'writes:'\n",
    ".*Newsgroups:.*?\\n|               # Ignora cualquier línea que contenga 'Newsgroups:'\n",
    "[^a-zA-Z0-9\\s.,]                  # Elimina cualquier carácter que no sea una letra, un número o un espacio\n",
    "|^>+                              # Elimina '>' al inicio de una línea\n",
    "|\\s*>+                            # Elimina '>' seguido por espacios\n",
    "^-+$                              # Ignora las líneas que contienen solo '-'\n",
    "^=+$                              # Ignora las líneas que contienen solo '='\n",
    "'''\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    def __init__(self, stop_words: Iterable[str] = None, lemmatizer: WordNetLemmatizer = None):\n",
    "        self.stop_words = stop_words or set(stopwords.words('english'))\n",
    "        self.lemmatizer = lemmatizer or WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return pd.Series([self.preprocess_document(doc) for doc in X])\n",
    "\n",
    "    def preprocess_document(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        tokens = [self.lemmatizer.lemmatize(token.lower()) for token in tokens if\n",
    "                  token.lower() not in self.stop_words and token.isalnum()]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "def load_news_documents(dataset_path: Path):\n",
    "    document_names = []\n",
    "    document_texts = []\n",
    "    document_classes = []\n",
    "    dataset_classnames = os.listdir(dataset_path)\n",
    "    for doc_class in dataset_classnames:\n",
    "        class_path = os.path.join(dataset_path, doc_class)\n",
    "        file_names = os.listdir(class_path)\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(class_path, file_name)\n",
    "            document_names.append(file_name)\n",
    "            document_classes.append(doc_class)\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as f:\n",
    "                # document_texts.append(f.read())\n",
    "                text = f.read()\n",
    "                cleaned_text = re.sub(twenty_n_regex, '', text, flags=re.VERBOSE | re.MULTILINE).replace(\"rh\",\"\")\n",
    "                document_texts.append(cleaned_text)\n",
    "    return pd.DataFrame(\n",
    "        {'document_name': document_names, 'document_class': document_classes, 'document_text': document_texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e7ccd382e602d2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:10:46.483913Z",
     "start_time": "2024-09-14T17:10:46.239894Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_news_documents('data/20news-18828/20news-18828')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38be6f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_name</th>\n",
       "      <th>document_class</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49960</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>\\n\\n                              Atheist Reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51060</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>\\n\\nBEGIN PGP SIGNED MESSAGE\\n\\n              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51119</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>\\n \\nWell, John has a quite different, not nec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51120</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>\\n      Recently, RAs have been ordered and no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51121</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>\\n\\n 1 HOWEVER, I hate economic terrorism and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18823</th>\n",
       "      <td>84564</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>\\n\\n     I wasnt sure if this was the right ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18824</th>\n",
       "      <td>84565</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>\\n  Probably not.  But then, I dont pack heavy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18825</th>\n",
       "      <td>84568</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>\\nIf you would like to understand better the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18826</th>\n",
       "      <td>84569</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>\\n\\n\\nThe danger of anticult groups is that wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18827</th>\n",
       "      <td>84570</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>\\n 34mAnd now . . . 35mDeep Thoughts0m\\n \\t32m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18828 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      document_name      document_class  \\\n",
       "0             49960         alt.atheism   \n",
       "1             51060         alt.atheism   \n",
       "2             51119         alt.atheism   \n",
       "3             51120         alt.atheism   \n",
       "4             51121         alt.atheism   \n",
       "...             ...                 ...   \n",
       "18823         84564  talk.religion.misc   \n",
       "18824         84565  talk.religion.misc   \n",
       "18825         84568  talk.religion.misc   \n",
       "18826         84569  talk.religion.misc   \n",
       "18827         84570  talk.religion.misc   \n",
       "\n",
       "                                           document_text  \n",
       "0      \\n\\n                              Atheist Reso...  \n",
       "1      \\n\\nBEGIN PGP SIGNED MESSAGE\\n\\n              ...  \n",
       "2      \\n \\nWell, John has a quite different, not nec...  \n",
       "3      \\n      Recently, RAs have been ordered and no...  \n",
       "4      \\n\\n 1 HOWEVER, I hate economic terrorism and ...  \n",
       "...                                                  ...  \n",
       "18823  \\n\\n     I wasnt sure if this was the right ne...  \n",
       "18824  \\n  Probably not.  But then, I dont pack heavy...  \n",
       "18825  \\nIf you would like to understand better the s...  \n",
       "18826  \\n\\n\\nThe danger of anticult groups is that wh...  \n",
       "18827  \\n 34mAnd now . . . 35mDeep Thoughts0m\\n \\t32m...  \n",
       "\n",
       "[18828 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3cd6e89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_name</th>\n",
       "      <th>document_class</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>54261</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>\\n\\n Organization University of Southern Queen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>54261</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>\\n\\n\\tESPN through a fortunate rainout of a ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>54261</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>\\n Is there a readily available solvent that d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15786</th>\n",
       "      <td>54261</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>\\n\\n\\n The issue has never been whether tanks ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      document_name      document_class  \\\n",
       "790           54261         alt.atheism   \n",
       "10450         54261    rec.sport.hockey   \n",
       "12479         54261     sci.electronics   \n",
       "15786         54261  talk.politics.guns   \n",
       "\n",
       "                                           document_text  \n",
       "790    \\n\\n Organization University of Southern Queen...  \n",
       "10450  \\n\\n\\tESPN through a fortunate rainout of a ba...  \n",
       "12479  \\n Is there a readily available solvent that d...  \n",
       "15786  \\n\\n\\n The issue has never been whether tanks ...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset.document_name == \"54261\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ba54b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Organization University of Southern Queensland\n",
      "\n",
      "\n",
      "\tFirst I want to start right out and say that Im a Christian.  It \n",
      "\n",
      " I know I shouldnt get involved, but...   \n",
      "\n",
      " bit deleted\n",
      "\n",
      "\tThe book says that Jesus was either a liar, or he was crazy  a \n",
      "modern day Koresh or he was actually who he said he was.\n",
      "rest of rant deleted\n",
      "\n",
      "This is a standard argument for fundies.  Can you spot the falicy The\n",
      "statement is arguing from the assumption that Jesus actually existed.  So far,\n",
      "they have not been able to offer real proof of that existance.  Most of them\n",
      "try it using the very flawed writings of Josh McDowell and others to prove\n",
      "it, but those writers use VERY flawed sources.  If they are real sources at\n",
      "all, some are not.  When will they ever learn to do real research, instead of\n",
      "believing the drivel sold in the Christian bookstores.\n",
      "\n",
      " Righto, DAN, try this one with your Cornflakes...\n",
      "\n",
      " The book says that Muhammad was either a liar, or he was\n",
      " crazy  a  modern day Mad Mahdi or he was actually who he\n",
      " said he was. Some reasons why he wouldnt be a liar are as\n",
      " follows.  Who would  die for a lie  Wouldnt people be able\n",
      " to tell if he was a liar  People  gathered around him and\n",
      " kept doing it, many gathered from hearing or seeing  how his\n",
      " soninlaw made the sun stand still.  Call me a fool, but I\n",
      " believe  he did make the sun stand still.  \n",
      " Niether was he a lunatic.  Would more than an entire nation\n",
      " be drawn  to someone who was crazy.  Very doubtful, in fact\n",
      " rediculous.  For example  anyone who is drawn to the Mad\n",
      " Mahdi is obviously a fool, logical people see  this right\n",
      " away.\n",
      " Therefore since he wasnt a liar or a lunatic, he must have\n",
      " been the  real thing.  \n",
      "\n",
      "Nice rebutal\n",
      "\n",
      "                   Alan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[dataset.document_name == \"54261\"][\"document_text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e18b413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "T.O. Radzykewycz writes\n",
      "\n",
      "   666, the file permission of the beast.\n",
      " \n",
      "  Sorry, but the file permission of the beast is 600.\n",
      "  \n",
      "  And the file permission of the home directory of the\n",
      "  beast is 700.\n",
      " \n",
      " Hey, radzy, it must depend on your systems access policy.\n",
      " I get\n",
      " \t ls lg usrusers\n",
      " \ttotal 3\n",
      " \tdrwxrwxrwx 22 beast    system       1536 Jan 01  1970 beast\n",
      " \tdrwxrxx 32 boylan   users        2048 Mar 31 0908 boylan\n",
      " \tdrwxrxrx  2 guest    users         512 Sep 18  1992 guest\n",
      " \t su\n",
      " \tPassword\n",
      " \troot  su beast\n",
      " \tbeast  umask\n",
      " \t111\n",
      " \tbeast  D\n",
      " \troot  D\n",
      " \t \n",
      " \n",
      " Just a minute....\n",
      " \n",
      " \t grep beast etcpasswd\n",
      " \tbeastk5tUk76RAUogQ4970Not Walt Disneyusrusersbeast\n",
      " \t mv usrusersbeast.profile usrusersbeast.profile,\n",
      " \t echo umask 077  usrusersbeast.profile\n",
      " \t cat  usrusersbeast.profile\n",
      " \tchmod 700 usrusersbeast\n",
      " \tmv .mailrc .mailrc,\n",
      " \tmv .mailrc, .mailrc\n",
      " \tmv usrusersbeast.profile, usrusersbeast.profile\n",
      " \tD\n",
      " \t chmod 777 usrusersbeast.profile\n",
      " \t cat usrusersbeast.profile,  usrusersbeast.profile\n",
      " \n",
      " waits a while, finally gets mail.\n",
      " \n",
      " I think you made a mistake.  Check it again.\n",
      " \n",
      "\n",
      "I see . . . youre not running Ultrix\n",
      "\n",
      "\t\n",
      "\n",
      "\t\t\t\t  Steve\n",
      "\n",
      "\n",
      "\n",
      "Dont miss the 49th New England Folk Festival,\n",
      "April 2325, 1993 in Natick, Massachusetts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[dataset.document_name == \"82763\"][\"document_text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc91de2cc444955",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:10:48.055884Z",
     "start_time": "2024-09-14T17:10:48.047432Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, remaining_df = train_test_split(dataset, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(remaining_df, test_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "699b867b1c1bf3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:10:49.774958Z",
     "start_time": "2024-09-14T17:10:49.769746Z"
    }
   },
   "outputs": [],
   "source": [
    "document_preprocessor = DocumentPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef25e49da6319192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:11:19.371456Z",
     "start_time": "2024-09-14T17:10:51.100171Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = document_preprocessor.transform(train_df['document_text'])\n",
    "X_val = document_preprocessor.transform(val_df['document_text'])\n",
    "X_test = document_preprocessor.transform(test_df['document_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71895b9eea6be7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:11:29.902308Z",
     "start_time": "2024-09-14T17:11:29.896960Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train_df['document_class']\n",
    "y_val = val_df['document_class']\n",
    "y_test = test_df['document_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad728ad0a032b68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:58:01.778778Z",
     "start_time": "2024-09-14T16:57:45.279413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8120021242697822"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_c_lr = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline_c_lr.fit(X_train, y_train)\n",
    "pipeline_c_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67f3435a3d8a5349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:58:16.186252Z",
     "start_time": "2024-09-14T16:58:04.473726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452823508585591"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_tfidf_lr = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline_tfidf_lr.fit(X_train, y_train)\n",
    "pipeline_tfidf_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da5df2eadcf58175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:11:43.166652Z",
     "start_time": "2024-09-14T17:11:43.163339Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_sparce = FunctionTransformer(lambda X: np.asarray(X.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5ca634ef1c61462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:59:14.624519Z",
     "start_time": "2024-09-14T16:59:08.696666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6877323420074349"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_c_nb = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=10000)),\n",
    "    ('to_dense', transform_sparce),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "pipeline_c_nb.fit(X_train, y_train)\n",
    "pipeline_c_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc7923796d2b0497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T16:59:19.339583Z",
     "start_time": "2024-09-14T16:59:14.635048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6893255443441317"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_tfidf_nb = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_features=10000)),\n",
    "    ('to_dense', transform_sparce),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "pipeline_tfidf_nb.fit(X_train, y_train)\n",
    "pipeline_tfidf_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ced0892569abf79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:27:18.686165Z",
     "start_time": "2024-09-14T17:24:04.922195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'classifier__C': 100}\n",
      "Best cross-validation score: 0.915851195352476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "pipeline_tfidf_lr = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(pipeline_tfidf_lr, param_grid, cv=kf, scoring='accuracy')\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bded970eb8c8b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:23:22.918671Z",
     "start_time": "2024-09-14T17:22:09.003891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'classifier__var_smoothing': 1e-08}\n",
      "Best cross-validation score: 0.7517262873846502\n"
     ]
    }
   ],
   "source": [
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__var_smoothing': [1e-10, 1e-9, 1e-8]\n",
    "}\n",
    "\n",
    "pipeline_tfidf_nb = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(max_features=10000)),\n",
    "    ('to_dense', transform_sparce),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "kf_nb = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "grid_search_nb = GridSearchCV(pipeline_tfidf_nb, param_grid, cv=kf_nb, scoring='accuracy')\n",
    "grid_search_nb.fit(X_train_val, y_train_val)\n",
    "print(f\"Best parameters found: {grid_search_nb.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search_nb.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62bee7cde68aa475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:36:53.387346Z",
     "start_time": "2024-09-14T17:36:53.383524Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print(f\"Precision (Macro): {precision_macro}\")\n",
    "    print(f\"Recall (Macro): {recall_macro}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro}\")\n",
    "    print(f\"Precision (Micro): {precision_micro}\")\n",
    "    print(f\"Recall (Micro): {recall_micro}\")\n",
    "    print(f\"F1 Score (Micro): {f1_micro}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "286bfbb18fb05670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:36:55.807528Z",
     "start_time": "2024-09-14T17:36:55.356186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (Macro): 0.919615441822286\n",
      "Recall (Macro): 0.9168014513574156\n",
      "F1 Score (Macro): 0.9178375068474682\n",
      "Precision (Micro): 0.9201628606833068\n",
      "Recall (Micro): 0.9201628606833068\n",
      "F1 Score (Micro): 0.9201628606833068\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(grid_search.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9b0fb8e41ccb15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:37:15.290407Z",
     "start_time": "2024-09-14T17:37:10.867972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (Macro): 0.762674604856379\n",
      "Recall (Macro): 0.7648273062076675\n",
      "F1 Score (Macro): 0.7611843493506316\n",
      "Precision (Micro): 0.7663303239511418\n",
      "Recall (Micro): 0.7663303239511418\n",
      "F1 Score (Micro): 0.7663303239511418\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(grid_search_nb.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23523c9244d8195f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T17:28:04.495827Z",
     "start_time": "2024-09-14T17:28:04.492366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
