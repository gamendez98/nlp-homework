{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_to_document(document_representation: str):\n",
    "    '''\n",
    "    Args:\n",
    "        document_representation: \n",
    "\n",
    "    Returns:\n",
    "        document_text, document_label\n",
    "    '''\n",
    "    word_counts = [term_representation.split(':') for term_representation in document_representation.split(' ')]\n",
    "    _, label = word_counts[-1]\n",
    "    word_counts = word_counts[:-1]\n",
    "    words = []\n",
    "    for term, count in word_counts:\n",
    "        words.extend([term] * int(count))\n",
    "    return ' '.join(words), label.replace(\"\\n\",\"\")\n",
    "\n",
    "def load_documents(file_path: Path):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        file_path: \n",
    "\n",
    "    Returns:\n",
    "        (document_texts, document_labels)\n",
    "    '''\n",
    "    with open(file_path, \"r\") as f:\n",
    "        document_representations = f.readlines()\n",
    "    return list(zip(*(representation_to_document(representation) for representation in document_representations)))\n",
    "\n",
    "def load_domain(domain_path: Path):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        domain_path: \n",
    "\n",
    "    Returns:\n",
    "        train_df, val_df\n",
    "    '''\n",
    "    positive_document_counts, positive_document_labels = load_documents(domain_path / \"positive.review\")\n",
    "    negative_documents_counts, negative_documents_labels = load_documents(domain_path / \"negative.review\")\n",
    "    unlabeled_documents_counts, unlabeled_documents_labels = load_documents(domain_path / \"unlabeled.review\")\n",
    "    return (\n",
    "        pd.DataFrame({\n",
    "            'sentiment': positive_document_labels + negative_documents_labels,\n",
    "            'document': positive_document_counts + negative_documents_counts\n",
    "        }),\n",
    "        pd.DataFrame({\n",
    "            'sentiment': unlabeled_documents_labels,\n",
    "            'document': unlabeled_documents_counts\n",
    "        })\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_path = Path(\"data/Multi Domain Sentiment/processed_acl\")\n",
    "domain_folders = [folder for folder in domains_path.iterdir() if folder.is_dir()]\n",
    "domain_folder = domain_folders[0]\n",
    "train_df, val_df = load_domain(domain_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>holes must top_secret he center other_civilans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>i_think dr_dean reason oz oz medicine_which me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>woman_the contains_the fan_i alex_ross(superma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>hurricane these_pages lost_innocence both at_h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>while commented the_rise the_rise if if strong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>negative</td>\n",
       "      <td>only idiotic_anyone if_i mystery_writer rather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>negative</td>\n",
       "      <td>your well to_create peter bored_me inconsisten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>negative</td>\n",
       "      <td>favorable_reviews heard straight book/pamphlet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>negative</td>\n",
       "      <td>helpful this_one substance_and pages_devoted i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>negative</td>\n",
       "      <td>see_other written_by objective_as columnist si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment                                           document\n",
       "0     positive  holes must top_secret he center other_civilans...\n",
       "1     positive  i_think dr_dean reason oz oz medicine_which me...\n",
       "2     positive  woman_the contains_the fan_i alex_ross(superma...\n",
       "3     positive  hurricane these_pages lost_innocence both at_h...\n",
       "4     positive  while commented the_rise the_rise if if strong...\n",
       "...        ...                                                ...\n",
       "1995  negative  only idiotic_anyone if_i mystery_writer rather...\n",
       "1996  negative  your well to_create peter bored_me inconsisten...\n",
       "1997  negative  favorable_reviews heard straight book/pamphlet...\n",
       "1998  negative  helpful this_one substance_and pages_devoted i...\n",
       "1999  negative  see_other written_by objective_as columnist si...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiwordnet(path: Path):\n",
    "    sentiwordnet = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            data = line.strip().split('\\t')\n",
    "            if len(data) != 6:\n",
    "                continue\n",
    "            _, _, pos_score, neg_score, synset_terms, _ = data\n",
    "            pos_score = float(pos_score)\n",
    "            neg_score = float(neg_score)\n",
    "            terms = synset_terms.split()\n",
    "            for term in terms:\n",
    "                word = term.split('#')[0]\n",
    "                if word in sentiwordnet:\n",
    "                    sentiwordnet[word]['pos_score'].append(pos_score)\n",
    "                    sentiwordnet[word]['neg_score'].append(neg_score)\n",
    "                else:\n",
    "                    sentiwordnet[word] = {'pos_score': [pos_score], 'neg_score': [neg_score]}\n",
    "    # Average the scores\n",
    "    for word in sentiwordnet:\n",
    "        sentiwordnet[word]['pos_score'] = sum(sentiwordnet[word]['pos_score']) / len(sentiwordnet[word]['pos_score'])\n",
    "        sentiwordnet[word]['neg_score'] = sum(sentiwordnet[word]['neg_score']) / len(sentiwordnet[word]['neg_score'])\n",
    "    return sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiwordnet_path = Path(\"data/EN_Lexicons/SentiWordNet_3.0.0.txt\")\n",
    "sentiwordnet = load_sentiwordnet(sentiwordnet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess words\n",
    "def preprocess_word(word):\n",
    "    # Remove special tokens like <num>\n",
    "    word = re.sub(r'<.*?>', '', word)\n",
    "    # Replace underscores and dots with spaces\n",
    "    word = word.replace('_', ' ').replace('.', ' ')\n",
    "    # Split into individual words\n",
    "    words = word.split()\n",
    "    # Lemmatize each word\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from a document\n",
    "def extract_features(document):\n",
    "    pos_score_total = 0.0\n",
    "    neg_score_total = 0.0\n",
    "    pos_word_count = 0\n",
    "    neg_word_count = 0\n",
    "    max_pos_score = 0.0\n",
    "    max_neg_score = 0.0\n",
    "    words = document.split()\n",
    "    word_count = 0\n",
    "    for word in words:\n",
    "        processed_words = preprocess_word(word)\n",
    "        for w in processed_words:\n",
    "            word_count += 1\n",
    "            if w in sentiwordnet:\n",
    "                pos_score = sentiwordnet[w]['pos_score']\n",
    "                neg_score = sentiwordnet[w]['neg_score']\n",
    "                pos_score_total += pos_score\n",
    "                neg_score_total += neg_score\n",
    "                if pos_score > 0:\n",
    "                    pos_word_count += 1\n",
    "                    max_pos_score = max(max_pos_score, pos_score)\n",
    "                if neg_score > 0:\n",
    "                    neg_word_count += 1\n",
    "                    max_neg_score = max(max_neg_score, neg_score)\n",
    "    # Avoid division by zero\n",
    "    avg_pos_score = pos_score_total / word_count if word_count > 0 else 0\n",
    "    avg_neg_score = neg_score_total / word_count if word_count > 0 else 0\n",
    "    score_difference = pos_score_total - neg_score_total\n",
    "    score_ratio = (pos_score_total / neg_score_total) if neg_score_total != 0 else 0\n",
    "    features = {\n",
    "        'total_pos_score': pos_score_total,\n",
    "        'total_neg_score': neg_score_total,\n",
    "        'avg_pos_score': avg_pos_score,\n",
    "        'avg_neg_score': avg_neg_score,\n",
    "        'max_pos_score': max_pos_score,\n",
    "        'max_neg_score': max_neg_score,\n",
    "        'pos_word_count': pos_word_count,\n",
    "        'neg_word_count': neg_word_count,\n",
    "        'score_difference': score_difference,\n",
    "        'score_ratio': score_ratio,\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['features'] = train_df['document'].apply(extract_features)\n",
    "features_df = pd.DataFrame(train_df['features'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_pos_score</th>\n",
       "      <th>total_neg_score</th>\n",
       "      <th>avg_pos_score</th>\n",
       "      <th>avg_neg_score</th>\n",
       "      <th>max_pos_score</th>\n",
       "      <th>max_neg_score</th>\n",
       "      <th>pos_word_count</th>\n",
       "      <th>neg_word_count</th>\n",
       "      <th>score_difference</th>\n",
       "      <th>score_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.044326</td>\n",
       "      <td>14.276926</td>\n",
       "      <td>0.033218</td>\n",
       "      <td>0.029559</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>162</td>\n",
       "      <td>157</td>\n",
       "      <td>1.767400</td>\n",
       "      <td>1.123794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.180929</td>\n",
       "      <td>10.754816</td>\n",
       "      <td>0.045588</td>\n",
       "      <td>0.032297</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>129</td>\n",
       "      <td>96</td>\n",
       "      <td>4.426113</td>\n",
       "      <td>1.411547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.963393</td>\n",
       "      <td>1.489286</td>\n",
       "      <td>0.056613</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "      <td>5.474107</td>\n",
       "      <td>4.675659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.878322</td>\n",
       "      <td>10.471467</td>\n",
       "      <td>0.040402</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>-2.593145</td>\n",
       "      <td>0.752361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.724131</td>\n",
       "      <td>22.597125</td>\n",
       "      <td>0.061588</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>245</td>\n",
       "      <td>218</td>\n",
       "      <td>17.127007</td>\n",
       "      <td>1.757929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>49.873843</td>\n",
       "      <td>47.355842</td>\n",
       "      <td>0.052388</td>\n",
       "      <td>0.049744</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>315</td>\n",
       "      <td>271</td>\n",
       "      <td>2.518002</td>\n",
       "      <td>1.053172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>16.744027</td>\n",
       "      <td>18.862985</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.072550</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>98</td>\n",
       "      <td>90</td>\n",
       "      <td>-2.118957</td>\n",
       "      <td>0.887666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>15.453002</td>\n",
       "      <td>12.256164</td>\n",
       "      <td>0.081332</td>\n",
       "      <td>0.064506</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>68</td>\n",
       "      <td>73</td>\n",
       "      <td>3.196838</td>\n",
       "      <td>1.260835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>16.026036</td>\n",
       "      <td>15.322254</td>\n",
       "      <td>0.044271</td>\n",
       "      <td>0.042327</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>140</td>\n",
       "      <td>120</td>\n",
       "      <td>0.703782</td>\n",
       "      <td>1.045932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>23.444194</td>\n",
       "      <td>14.292318</td>\n",
       "      <td>0.047077</td>\n",
       "      <td>0.028699</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>178</td>\n",
       "      <td>135</td>\n",
       "      <td>9.151876</td>\n",
       "      <td>1.640335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_pos_score  total_neg_score  avg_pos_score  avg_neg_score  \\\n",
       "0           16.044326        14.276926       0.033218       0.029559   \n",
       "1           15.180929        10.754816       0.045588       0.032297   \n",
       "2            6.963393         1.489286       0.056613       0.012108   \n",
       "3            7.878322        10.471467       0.040402       0.053700   \n",
       "4           39.724131        22.597125       0.061588       0.035034   \n",
       "...               ...              ...            ...            ...   \n",
       "1995        49.873843        47.355842       0.052388       0.049744   \n",
       "1996        16.744027        18.862985       0.064400       0.072550   \n",
       "1997        15.453002        12.256164       0.081332       0.064506   \n",
       "1998        16.026036        15.322254       0.044271       0.042327   \n",
       "1999        23.444194        14.292318       0.047077       0.028699   \n",
       "\n",
       "      max_pos_score  max_neg_score  pos_word_count  neg_word_count  \\\n",
       "0          0.305556         0.6250             162             157   \n",
       "1          1.000000         0.5000             129              96   \n",
       "2          0.750000         0.1250              40              24   \n",
       "3          0.500000         0.6875              60              60   \n",
       "4          1.000000         0.6250             245             218   \n",
       "...             ...            ...             ...             ...   \n",
       "1995       0.750000         0.8750             315             271   \n",
       "1996       0.500000         0.7500              98              90   \n",
       "1997       0.575000         0.6250              68              73   \n",
       "1998       0.475000         0.6250             140             120   \n",
       "1999       0.446429         0.7500             178             135   \n",
       "\n",
       "      score_difference  score_ratio  \n",
       "0             1.767400     1.123794  \n",
       "1             4.426113     1.411547  \n",
       "2             5.474107     4.675659  \n",
       "3            -2.593145     0.752361  \n",
       "4            17.127007     1.757929  \n",
       "...                ...          ...  \n",
       "1995          2.518002     1.053172  \n",
       "1996         -2.118957     0.887666  \n",
       "1997          3.196838     1.260835  \n",
       "1998          0.703782     1.045932  \n",
       "1999          9.151876     1.640335  \n",
       "\n",
       "[2000 rows x 10 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Extract features for each document in the dataset (assuming X is a DataFrame with a 'document' column)\n",
    "        feature_list = X.apply(extract_features)\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        features_df = pd.DataFrame(feature_list.tolist())\n",
    "        return features_df\n",
    "    \n",
    "def create_fit_model(X_train, y_train, vectorizer, middleware, model):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('middleware', middleware),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    return {\n",
    "        \"Precision (Macro)\": precision_macro,\n",
    "        \"Recall (Macro)\": recall_macro,\n",
    "        \"F1 Score (Macro)\": f1_macro,\n",
    "        \"Precision (Micro)\": precision_micro,\n",
    "        \"Recall (Micro)\": recall_micro,\n",
    "        \"F1 Score (Micro)\": f1_micro,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_transformer = FunctionTransformer(lambda x: x)\n",
    "\n",
    "X_train = train_df['document']\n",
    "y_train = train_df['sentiment']\n",
    "X_test = val_df['document']\n",
    "y_test = val_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameter = {'vectorizer': CustomFeature(), 'middleware': identity_transformer, 'model': LogisticRegression(solver=\"liblinear\")}\n",
    "model_info = {\n",
    "    'vectorizer': type(model_parameter['vectorizer']).__name__,\n",
    "    'model': type(model_parameter['model']).__name__,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_fit_model(X_train, y_train, **model_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision (Macro)': 0.6843186945637729,\n",
       " 'Recall (Macro)': 0.6819218055397241,\n",
       " 'F1 Score (Macro)': 0.6802631049603733,\n",
       " 'Precision (Micro)': 0.6810750279955207,\n",
       " 'Recall (Micro)': 0.6810750279955207,\n",
       " 'F1 Score (Micro)': 0.6810750279955207}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;custom_feature&#x27;,\n",
       "                 &lt;__main__.CustomFeature object at 0x000001DA32AA6150&gt;),\n",
       "                (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(max_iter=500))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;custom_feature&#x27;,\n",
       "                 &lt;__main__.CustomFeature object at 0x000001DA32AA6150&gt;),\n",
       "                (&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;, LogisticRegression(max_iter=500))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CustomFeature</label><div class=\"sk-toggleable__content\"><pre>&lt;__main__.CustomFeature object at 0x000001DA32AA6150&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('custom_feature',\n",
       "                 <__main__.CustomFeature object at 0x000001DA32AA6150>),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('model', LogisticRegression(max_iter=500))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('custom_feature', CustomFeature()),  # Custom feature extraction\n",
    "    ('scaler', StandardScaler()),  # Scale the features\n",
    "    ('model', LogisticRegression(solver='lbfgs', max_iter=500))  # Logistic Regression\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Coefficients: [ 0.0020393  -0.00417675  0.50645297 -0.84635729  0.39502054 -0.42217585\n",
      " -0.12888608  0.12174955  0.01771441 -0.06495051]\n"
     ]
    }
   ],
   "source": [
    "# Access the trained logistic regression model\n",
    "logreg_model = pipeline.named_steps['model']\n",
    "\n",
    "# Get the coefficients (importance of each feature)\n",
    "coefficients = logreg_model.coef_\n",
    "\n",
    "# If you have a binary classification problem, coefficients[0] will be the array of feature importance\n",
    "print(\"Feature Coefficients:\", coefficients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: avg_neg_score, Coefficient: -0.8463572869277443\n",
      "Feature: avg_pos_score, Coefficient: 0.5064529745738771\n",
      "Feature: max_neg_score, Coefficient: -0.4221758538201927\n",
      "Feature: max_pos_score, Coefficient: 0.39502053927069697\n",
      "Feature: pos_word_count, Coefficient: -0.12888608404295168\n",
      "Feature: neg_word_count, Coefficient: 0.12174955313507597\n",
      "Feature: score_ratio, Coefficient: -0.06495050896835664\n",
      "Feature: score_difference, Coefficient: 0.017714413851078147\n",
      "Feature: total_neg_score, Coefficient: -0.004176753379259705\n",
      "Feature: total_pos_score, Coefficient: 0.002039302084932296\n"
     ]
    }
   ],
   "source": [
    "# Assuming your CustomFeature class outputs a DataFrame with these feature names\n",
    "feature_names = ['total_pos_score', 'total_neg_score', 'avg_pos_score', 'avg_neg_score',\n",
    "                 'max_pos_score', 'max_neg_score', 'pos_word_count', 'neg_word_count',\n",
    "                 'score_difference', 'score_ratio']\n",
    "\n",
    "# Combine feature names with corresponding coefficients\n",
    "feature_importance = zip(feature_names, coefficients[0])\n",
    "\n",
    "# Sort features by the absolute value of their coefficient (importance)\n",
    "sorted_features = sorted(feature_importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Display sorted feature importance\n",
    "for feature, coef in sorted_features:\n",
    "    print(f\"Feature: {feature}, Coefficient: {coef}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
