{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructuración y organización de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datasets utilizados son:\n",
    "\n",
    "- [20Newsgroups](http://qwone.com/~jason/20Newsgroups/)\n",
    "- [BAC: The Blog Authorship Corpus](https://huggingface.co/datasets/barilan/blog_authorship_corpus)\n",
    "\n",
    "Fueron descargados desde el link de Dropbox proporcionado, en el siguiente [enlace](https://huggingface.co/datasets/barilan/blog_authorship_corpus).\n",
    "\n",
    "Para el siguiente proceso vamos a suponer la siguiente estructura en el directorio.\n",
    "\n",
    "```\n",
    "data\n",
    "│\n",
    "└───20news-18828\n",
    "│   └───20news-18828\n",
    "│       └───alt.atheism\n",
    "│       └───comp.graphics\n",
    "│       └───...\n",
    "│\n",
    "└───BAC\n",
    "│   │   5114.male.25.indUnk.Scorpio.xml\n",
    "│   │   blogs.zip\n",
    "│\n",
    "└───EN_Lexicons\n",
    "│   │   AFINN-111.txt\n",
    "│   │   senticnet5.py\n",
    "│   │   SentiWordNet_3.0.0.txt\n",
    "│   │   WordStat Sentiments.txt\n",
    "│\n",
    "└───Multi Domain Sentiment\n",
    "│   │   negative.review\n",
    "│   │   processed_acl.tar.gz\n",
    "│   │   unlabeled.review\n",
    "```\n",
    "\n",
    "Nótese que el único paso adicional con respecto a los datos descargados de Dropbox es extraer el archivo `20news-18828.tar.gz`, que da lugar a la carpeta con el mismo nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, importamos algunas librerías de utilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, zipfile\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir las rutas para acceder a los textos en `BAC` y `20N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_n_path = os.path.join('data', '20news-18828', '20news-18828')\n",
    "bac_path = os.path.join('data', 'BAC', 'blogs.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los textos de `20N` se encuentran en un formato que es conveniente limpiar primero con expresiones `regex`. A continuación, definimos una expresión regular para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_n_regex = r'''\n",
    "^From:.*?\\n|                      # Ignora la línea que empieza con 'From:' y lo que sigue hasta el final de la línea\n",
    "^Subject:.*?\\n|                   # Ignora la línea que empieza con 'Subject:' y lo que sigue hasta el final de la línea\n",
    "^Archive-name:.*?\\n|              # Ignora la línea que empieza con 'Archive-name:' y lo que sigue hasta el final de la línea\n",
    "^Alt-atheism-archive-name:.*?\\n|  # Ignora la línea que empieza con 'Alt-atheism-archive-name:' y lo que sigue hasta el final de la línea\n",
    "^Last-modified:.*?\\n|             # Ignora la línea que empieza con 'Last-modified:' y lo que sigue hasta el final de la línea\n",
    "^Version:.*?\\n|                   # Ignora la línea que empieza con 'Version:' y lo que sigue hasta el final de la línea\n",
    "^.*@.*?\\n|                        # Ignora la línea que contiene '@' y lo que sigue hasta el final de la línea\n",
    "In\\sarticle.*?writes:\\n|          # Ignora todo lo que está entre 'In article...' y 'writes:'\n",
    "[^a-zA-Z0-9\\s.,]                  # Elimina cualquier carácter que no sea una letra, un número o un espacio\n",
    "|^>+                              # Elimina '>' al inicio de una línea\n",
    "|\\s*>+                            # Elimina '>' seguido por espacios\n",
    "^-+$                              # Ignora las líneas que contienen solo '-'\n",
    "^=+$                              # Ignora las líneas que contienen solo '='\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, definimos una función para listar los nombres de los directorios de una ruta en particular, y listar de la misma manera los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_directories(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retorna una lista de directorios dentro de la ruta especificada.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta en la que buscar subdirectorios.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de nombres de los subdirectorios.\n",
    "    \"\"\"\n",
    "    return [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "\n",
    "def list_files(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retorna una lista de nombres de archivos dentro de la ruta especificada.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta en la que buscar archivos.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de nombres de archivos\n",
    "    \"\"\"\n",
    "    return [name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes funciones obtienen los textos relevantes de `20N` y `BAC`. Nótese que reciben un archivo o la ruta a este, y extraen el texto que nos interesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_text_twenty_news(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae y limpia el texto relevante de un archivo utilizando regex.\n",
    "\n",
    "    Args:\n",
    "        path (str): La ruta del archivo a leer y procesar.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto limpio.\n",
    "    \"\"\"\n",
    "    global twenty_n_regex # Usar una expresión regular global predefinida\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        # Limpiar el texto utilizando regex\n",
    "        cleaned_text = re.sub(twenty_n_regex, '', text, flags=re.VERBOSE | re.MULTILINE)\n",
    "\n",
    "    # Eliminar espacios en blanco repetidos y retornar el texto\n",
    "    return ' '.join(cleaned_text.split())\n",
    "\n",
    "def get_relevant_text_bac(content: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extrae y limpia el texto relevante de datos BAC, incluyendo fechas y el texto en cuestión (las publicaciones).\n",
    "\n",
    "    Args:\n",
    "        content (str): El contenido del texto a procesar.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: Una lista de tuplas donde cada tupla contiene una fecha y su texto.\n",
    "    \"\"\"\n",
    "    # Extraer todas las fechas y publicaciones del contenido\n",
    "    dates = re.findall(r'<date>(.*?)</date>', content, re.DOTALL)\n",
    "    posts = re.findall(r'<post>(.*?)</post>', content, re.DOTALL)\n",
    "    \n",
    "    # Limpiar los textos eliminando espacios innecesarios y palabras irrelevantes\n",
    "    cleaned_posts = [' '.join(re.sub(r'\\s+', ' ', post).split()).replace('urlLink', '') for post in posts]\n",
    "    \n",
    "    # Combinar las fechas y los textos en tuplas y retornarlas\n",
    "    return list(zip(dates, cleaned_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes son las funciones de procesamiento principales para consolidar todos los archivos tanto de `BAC` como de `20N` en sus propios `txt` aparte. Nótese que generan archivos llamados `consolidated_bac.txt` y `consolidated_news.txt` en el mismo directorio en donde se ejecute este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_twenty_news_data(path: str) -> None:\n",
    "    \"\"\"\n",
    "    Procesa todos los datos de 20N, consolida el texto y lo escribe en un archivo.\n",
    "\n",
    "    Args:\n",
    "        ruta (str): La ruta del directorio que contiene los datos de noticias.\n",
    "    \"\"\"\n",
    "    with open('consolidated_news.txt', 'w', encoding='utf-8') as output_file:\n",
    "\n",
    "        # Listar los subdirectorios que contienen las noticias\n",
    "        news_dirs = list_directories(path)\n",
    "        \n",
    "        # Para cada subdirectorio\n",
    "        for each_news_dir in news_dirs:\n",
    "            each_path = os.path.join(path, each_news_dir) # Ruta completa del subdirectorio\n",
    "            news_files = list_files(each_path) # Listar archivos dentro del subdirectorio\n",
    "            \n",
    "            # Para cada archivo de noticias\n",
    "            for each_news_file in news_files:\n",
    "                file_path = os.path.join(each_path, each_news_file) # Ruta completa del archivo\n",
    "                text = get_relevant_text_twenty_news(file_path) # Extraer el texto relevante\n",
    "                output_file.write(text + '\\n') # Escribir el texto en el archivo de salida\n",
    "\n",
    "def process_bac_data(zip_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Procesa todos los datos de BAC desde un archivo zip, extrae información relevante y la escribe en un archivo.\n",
    "\n",
    "    Args:\n",
    "        ruta_zip (str): La ruta al archivo zip que contiene los datos de BAC.\n",
    "    \"\"\"\n",
    "    # Abrir el archivo zip para lectura\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_names = zip_ref.namelist() # Obtener la lista de nombres de archivos en el zip\n",
    "        \n",
    "        with open('consolidated_bac.txt', 'w', encoding='utf-8') as output_file:\n",
    "            # Para cada archivo en el zip\n",
    "            for file_name in file_names:\n",
    "                if not file_name.endswith('/'): # Ignorar directorios dentro del zip\n",
    "                    with zip_ref.open(file_name) as file:\n",
    "                        try:\n",
    "                            # Intentar leer el contenido del archivo como UTF-8\n",
    "                            content = file.read().decode('utf-8')\n",
    "                        except UnicodeDecodeError:\n",
    "                            # Si falla, leerlo como latin1\n",
    "                            content = file.read().decode('latin1')\n",
    "                        \n",
    "                        # Extraer las parejas de fecha y texto\n",
    "                        data_pairs = get_relevant_text_bac(content)\n",
    "                        \n",
    "                        # Escribir cada pareja en el archivo de salida\n",
    "                        for date, post in data_pairs:\n",
    "                            output_file.write(f\"{date} - {post}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, la siguiente celda ejecuta estos procesos y genera los archivos consolidados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_twenty_news_data(twenty_n_path)\n",
    "process_bac_data(bac_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
