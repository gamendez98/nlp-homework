{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de modelos de lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, math, nltk\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para cargar los modelos generados, y para leer y tokenizar las oraciones de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ngram_model(filename: str) -> Dict[Tuple[str, ...], float]:\n",
    "    \"\"\"\n",
    "    Carga un modelo de n-gramas desde un archivo binario guardado con pickle.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Nombre del archivo desde el que se cargará el modelo.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[Tuple[str, ...], float]: Diccionario con las probabilidades de los n-gramas cargado desde el archivo.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        ngram_probabilities = pickle.load(file)\n",
    "    return ngram_probabilities\n",
    "\n",
    "def read_and_tokenize(file_path: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Lee un archivo de texto, tokeniza las oraciones y retorna una lista de listas de tokens.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Ruta al archivo de texto.\n",
    "        \n",
    "    Returns:\n",
    "        List[List[str]]: Lista de listas de tokens (cada oración tokenizada).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sentences = text.strip().split('\\n')\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos modelos para ambos datasets, y leemos las oraciones de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probs_20N = load_ngram_model(\"20N_unigrams.pkl\")\n",
    "bigram_probs_20N = load_ngram_model(\"20N_bigrams.pkl\")\n",
    "trigram_probs_20N = load_ngram_model(\"20N_trigrams.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_probs_BAC = load_ngram_model(\"BAC_unigrams.pkl\")\n",
    "bigram_probs_BAC = load_ngram_model(\"BAC_bigrams.pkl\")\n",
    "trigram_probs_BAC = load_ngram_model(\"BAC_trigrams.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences_20N = read_and_tokenize(\"20N_testing.txt\")\n",
    "test_sentences_BAC = read_and_tokenize(\"BAC_testing.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Cálculo de la perplejidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fórmula vista de perplejidad es la siguiente:\n",
    "\n",
    "$$PP(S)=\\left(\\prod_{i=1}^T P\\left(w_i \\mid w_{i-n} \\dots w_{i-1 }\\right)\\right)^{-1 / T}$$\n",
    "\n",
    "Como las probabilidades manejadas en nuestros modelos de lenguaje son de orden muy pequeños, podemos usar las probabilidades en espacio logaritmico de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log (P P(s)) & =\\log \\left(\\left(\\prod_{i=1}^T P\\left(w_i \\mid w_{i-n} \\dots w_{i-1 }\\right)\\right)^{-1 / T}\\right) \\\\\n",
    "& =-\\frac{1}{T} \\log \\left(\\prod_{i=1}^T P\\left(w_i \\mid w_{i-n} \\dots w_{i-1}\\right)\\right) \\\\\n",
    "& = -\\frac{1}{T} \\sum_{i=1}^T \\log \\left( P\\left(w_i \\mid w_{i-n} \\dots w_{i-1}\\right) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Así,\n",
    "\n",
    "$$PP(S)= \\exp \\left[  -\\frac{1}{T} \\sum_{i=1}^T \\log \\left( P\\left(w_i \\mid w_{i-n} \\dots w_{i-1}\\right) \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función implementa este razonamiento para un listado de oraciones de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_log(model: Dict[Tuple[str, ...], float], test_sentences: List[List[str]], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo de lenguaje n-gramas, utilizando logaritmos para evitar overflow.\n",
    "\n",
    "    Args:\n",
    "        model (Dict[Tuple[str, ...], float]): modelo de n-gramas. Es un diccionario que contiene las probabilidades de los n-gramas.\n",
    "        test_sentences (List[List[str]]): Lista de oraciones tokenizadas (listas de tokens) para probar el modelo.\n",
    "        n (int): Tamaño del n-grama a utilizar.\n",
    "    \n",
    "    Returns:\n",
    "        float: El valor de la perplejidad del modelo sobre las oraciones de prueba.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0\n",
    "    total_ngrams = 0\n",
    "    \n",
    "    # Para cada oración en el conjunto de prueba\n",
    "    for sentence in test_sentences:\n",
    "        sentence_log_prob = 0\n",
    "        ngram_count = 0\n",
    "        \n",
    "        # Genera los n-gramas de la oración actual\n",
    "        n_grams = list(nltk.ngrams(sentence, n))\n",
    "\n",
    "        for ngram in n_grams:\n",
    "            if ngram in model:\n",
    "                prob = model[ngram]\n",
    "            else:\n",
    "                prob = 1e-10  # Si el n-grama no está en el modelo, asignar una probabilidad baja. Así evitamos el logaritmo de 0 (indefinido).\n",
    "\n",
    "            # Sumar el logaritmo de la probabilidad del n-grama\n",
    "            sentence_log_prob += math.log(prob)\n",
    "            ngram_count += 1\n",
    "        \n",
    "        # Sumar la probabilidad (en log) total de la oración a la probabilidad total\n",
    "        total_log_prob += sentence_log_prob\n",
    "        total_ngrams += ngram_count\n",
    "\n",
    "    # Promedio de la probabilidad\n",
    "    avg_log_prob = total_log_prob / total_ngrams\n",
    "    \n",
    "    # La perplejidad es el exponente negativo del promedio\n",
    "    perplexity = math.exp(-avg_log_prob)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20N Dataset results:\n",
      "\n",
      "Perplejidad del modelo de unigramas: 963.8673012218808\n",
      "Perplejidad del modelo de bigramas: 11517.645242993816\n",
      "Perplejidad del modelo de trigramas: 1499439.081950886\n"
     ]
    }
   ],
   "source": [
    "print(\"20N Dataset results:\\n\")\n",
    "\n",
    "perplexity_unigrams_20N = calculate_perplexity_log(unigram_probs_20N, test_sentences_20N, n=1)\n",
    "print(f'Perplejidad del modelo de unigramas: {perplexity_unigrams_20N}')\n",
    "\n",
    "perplexity_bigrams_20N = calculate_perplexity_log(bigram_probs_20N, test_sentences_20N, n=2)\n",
    "print(f'Perplejidad del modelo de bigramas: {perplexity_bigrams_20N}')\n",
    "\n",
    "perplexity_trigrams_20N = calculate_perplexity_log(trigram_probs_20N, test_sentences_20N, n=3)\n",
    "print(f'Perplejidad del modelo de trigramas: {perplexity_trigrams_20N}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAC Dataset results:\n",
      "\n",
      "Perplejidad del modelo de unigramas: 764.772774714158\n",
      "Perplejidad del modelo de bigramas: 1885.4522528755106\n",
      "Perplejidad del modelo de trigramas: 217254.66092308247\n"
     ]
    }
   ],
   "source": [
    "print(\"BAC Dataset results:\\n\")\n",
    "\n",
    "perplexity_unigrams_BAC = calculate_perplexity_log(unigram_probs_BAC, test_sentences_BAC, n=1)\n",
    "print(f'Perplejidad del modelo de unigramas: {perplexity_unigrams_BAC}')\n",
    "\n",
    "perplexity_bigrams_BAC = calculate_perplexity_log(bigram_probs_BAC, test_sentences_BAC, n=2)\n",
    "print(f'Perplejidad del modelo de bigramas: {perplexity_bigrams_BAC}')\n",
    "\n",
    "perplexity_trigrams_BAC = calculate_perplexity_log(trigram_probs_BAC, test_sentences_BAC, n=3)\n",
    "print(f'Perplejidad del modelo de trigramas: {perplexity_trigrams_BAC}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función define la estrategia de generación de texto, dado un modelo de n-gramas, la oración inicial, la cantidad de gramas considerados, y la longitud máxima del texto a generar. Con excepción de unigramas, que simplemente agarra el token con mayor probabilidad, la estrategia de generación busca el contexto dado en el modelo para generar el siguiente token, extrayendo por simplicidad aquel con mayor probabilidad. La generación se detiene si:\n",
    "\n",
    "- Se llega a `max_length`\n",
    "- Se llega al token `\"</s>\"`\n",
    "- No hay gramas posibles como continuación de la oración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: Dict[Tuple[str, ...], float], start_sentence: str, n: int, max_length: int = 20) -> str:\n",
    "    \"\"\"\n",
    "    Genera texto usando el modelo n-grama basado en el n-grama de mayor probabilidad.\n",
    "\n",
    "    Args:\n",
    "        model: Diccionario de n-gramas con sus probabilidades.\n",
    "        start_sentence: Frase de inicio como string.\n",
    "        max_length: Longitud máxima de la oración generada.\n",
    "        n: Tamaño del n-grama (ej. 1 para unigramas).\n",
    "\n",
    "    Returns:\n",
    "        string: Oración generada.\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = start_sentence.split()\n",
    "\n",
    "    if n == 1:\n",
    "        # Para unigramas, seleccionamos el token con la mayor probabilidad en cada paso.\n",
    "        for _ in range(max_length):\n",
    "            next_token = max(model, key=model.get)\n",
    "            if next_token == '</s>':\n",
    "                break\n",
    "            sentence.append(next_token[0])\n",
    "    else:\n",
    "        # Para n-gramas donde n > 1\n",
    "        for _ in range(max_length):\n",
    "            context_size = min(len(sentence), n-1)  # Extraer cuantos tokens de contexto tenemos\n",
    "            context = tuple(sentence[-context_size:])  # Tomar el contexto disponible\n",
    "            \n",
    "            # Buscar las posibles continuaciones en el modelo\n",
    "            possible_ngrams = [ngram for ngram in model if ngram[:context_size] == context]\n",
    "            \n",
    "            # Si no hay posibles continuaciones, detenemos la generación\n",
    "            if not possible_ngrams:\n",
    "                break\n",
    "            \n",
    "            # Seleccionar el n-grama con la mayor probabilidad\n",
    "            next_token = max(possible_ngrams, key=lambda ngram: model[ngram])[-1]\n",
    "\n",
    "            # Si llegamos al token de fin de oración, detener la generación\n",
    "            if next_token == '</s>':\n",
    "                break\n",
    "            sentence.append(next_token)\n",
    "\n",
    "            # Si hemos generado suficientes palabras, detener la generación\n",
    "            if len(sentence) >= max_length:\n",
    "                break\n",
    "    \n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> s NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM NUM'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(trigram_probs_BAC, \"<s>\", max_length= 50, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> this is the best of all the time'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(trigram_probs_BAC, \"<s> this\", max_length= 50, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do you think you re not going to be a good thing'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(trigram_probs_BAC, \"how do\", max_length= 50, n = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
