{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tarea 2 - Modelos N-Gramas"
      ],
      "metadata": {
        "id": "Rt4tJaKh0wfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from typing import List\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk import PorterStemmer, word_tokenize, sent_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Descargar recursos necesarios de NLTK\n",
        "nltk.download('punkt')  # Tokenizador\n",
        "\n",
        "# Configuración de NLTK\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MUdnQN4SMu8K",
        "outputId": "a7bc0730-81f6-47f7-d3b9-8d5a74d91e4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de datos"
      ],
      "metadata": {
        "id": "RYxZYluyDG6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones auxiliares"
      ],
      "metadata": {
        "id": "OxGeRGrzDOWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_preprocessing(tokens: List[str], token_counts: Counter) -> List[str]:\n",
        "    \"\"\"\n",
        "    Realiza el preprocesamiento de texto, aplicando las siguientes transformaciones:\n",
        "    1. Convierte todos los tokens a minúsculas.\n",
        "    3. Elimina tokens que no son palabras (puntuación).\n",
        "    3. Convierte tokens de números al token NUM\n",
        "    4. Aplica stemming con PorterStemmer.\n",
        "    5. Reemplaza tokens con frecuencia 1 con <UNK>.\n",
        "\n",
        "    Args:\n",
        "        tokens (List[str]): Lista de tokens a preprocesar.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Lista de tokens preprocesados.\n",
        "    \"\"\"\n",
        "    # Pasa los tokens a minúsculas\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    tokens = [re.sub(r'\\W', '', word) for word in tokens]\n",
        "    tokens = [re.sub(r'\\s+[a-zA-Z]\\s+', '', word) for word in tokens]\n",
        "    tokens = [re.sub(r'\\^[a-zA-Z]\\s+', '', word) for word in tokens]\n",
        "\n",
        "    # Sustituir números por el Token 'NUM'\n",
        "    tokens = [re.sub(r'[0-9]+', 'NUM', word) for word in tokens]\n",
        "\n",
        "    tokens = [word for word in tokens if len(word) > 0]\n",
        "\n",
        "    # Aplica stemming\n",
        "    ps = PorterStemmer()\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "\n",
        "    # Reemplazar tokens con frecuencia 1 con <UNK>\n",
        "    tokens = [word if token_counts[word] > 1 else '<UNK>' for word in tokens]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "3Ajjnp47NAcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdwNQziBmZO8"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Preprocesa un texto mediante la tokenización y el preprocesamiento de tokens.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texto a preprocesar.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Lista de tokens preprocesados.\n",
        "    \"\"\"\n",
        "    # Tokenizar el texto en frases\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenizar todas las frases en palabras y aplanar la lista para contar frecuencias\n",
        "    all_tokens = [word.lower() for sentence in sentences for word in word_tokenize(sentence)]\n",
        "\n",
        "    # Contar la frecuencia de los tokens en todo el texto\n",
        "    token_counts = Counter(all_tokens)\n",
        "\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Añadir las etiquetas de inicio y fin de oración\n",
        "        tokens = [\"<s>\"] + word_tokenize(sentence) + [\"</s>\"]\n",
        "\n",
        "        # Preprocesar los tokens\n",
        "        processed_tokens = token_preprocessing(tokens, token_counts)\n",
        "\n",
        "        processed_tokens[0] = '<s>'\n",
        "        processed_tokens[-1] = '</s>'\n",
        "        processed_sentences.append(processed_tokens)\n",
        "\n",
        "    return processed_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos nuestros datasets"
      ],
      "metadata": {
        "id": "sfILMrX5NVKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_txt_files(file_path: str, csv_file_path: str, chunksize: int = 1000):\n",
        "    \"\"\"\n",
        "    Procesa un archivo .txt línea por línea en chunks y guarda los resultados en un archivo CSV.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Ruta al archivo .txt a procesar.\n",
        "        csv_file_path (str): Ruta al archivo .csv donde se guardarán los resultados.\n",
        "        chunksize (int): Número de líneas a procesar en cada chunk.\n",
        "    \"\"\"\n",
        "    chunk_list = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            lines = []\n",
        "            for i, line in enumerate(file):\n",
        "                lines.append(line.strip())\n",
        "                if (i + 1) % chunksize == 0:\n",
        "                    # Procesar el chunk actual\n",
        "                    processed_sentences = [text_preprocessing(l) for l in lines]\n",
        "                    flat_sentences = [\" \".join(sentence) for sentence_chunk in processed_sentences for sentence in sentence_chunk]\n",
        "\n",
        "                    # Crear un DataFrame con el resultado\n",
        "                    chunk_df = pd.DataFrame({'processed': flat_sentences})\n",
        "                    chunk_list.append(chunk_df)\n",
        "\n",
        "                    # Guardar en CSV\n",
        "                    chunk_df.to_csv(csv_file_path, mode='a', header=not pd.io.common.file_exists(csv_file_path), index=False)\n",
        "                    lines = []  # Reset the list\n",
        "\n",
        "            if lines:  # Procesar las líneas restantes si las hay\n",
        "                processed_sentences = [text_preprocessing(l) for l in lines]\n",
        "                flat_sentences = [\" \".join(sentence) for sentence_chunk in processed_sentences for sentence in sentence_chunk]\n",
        "\n",
        "                chunk_df = pd.DataFrame({'processed': flat_sentences})\n",
        "                chunk_df.to_csv(csv_file_path, mode='a', header=not pd.io.common.file_exists(csv_file_path), index=False)"
      ],
      "metadata": {
        "id": "Odyh523TEuX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cvs(file_path: str, tokens: List[str]):\n",
        "  with open(file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write('tokens\\n')\n",
        "    for sentence in tokens:\n",
        "      file.write(' '.join(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "hd-zLFQJDZde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generar CSV"
      ],
      "metadata": {
        "id": "lbVbItNwde4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'data/consolidated_news.txt'\n",
        "process_txt_files(directory, 'data/token_news.csv')"
      ],
      "metadata": {
        "id": "DP7uogPZDSuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'data/consolidated_bac.txt'\n",
        "process_txt_files(directory, 'data/token_bac.csv')"
      ],
      "metadata": {
        "id": "p2IUuGrxDTqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separar en train y test"
      ],
      "metadata": {
        "id": "9XzITCEdDdDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "GbMhnWKGDf_A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_twenty_news = pd.read_csv('data/token_news.csv')"
      ],
      "metadata": {
        "id": "TAKBtIU1DmLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8f4d40e4-e221-47bc-93bb-6737c57b874d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/token_news.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b7f04d45cd7d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_twenty_news\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/token_news.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/token_news.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_twenty_news.shape"
      ],
      "metadata": {
        "id": "dOBabCuwDqBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_twenty_news.columns = ['text']"
      ],
      "metadata": {
        "id": "MhyeR9_6J7f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(df_twenty_news['text'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7tQa4BWiDwqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.to_csv('data/20N_training', index=False, header=False)\n",
        "X_test.to_csv('data/20N_testing.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "tb_Exkv4KQC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos"
      ],
      "metadata": {
        "id": "dHj-YJNkz405"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proceso para el dataset 20N"
      ],
      "metadata": {
        "id": "haHMTpI93uUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga"
      ],
      "metadata": {
        "id": "drJu4piCz-99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_twenty_news = pd.read_csv('data/token_news.csv')"
      ],
      "metadata": {
        "id": "XjaAJlGr0Aik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_twenty_news.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH8Kbar70B3b",
        "outputId": "a501aa93-f52d-49ee-8d48-4a9719008143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(471748, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_twenty_news.columns = ['text']"
      ],
      "metadata": {
        "id": "fcgG2dwI0Dt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(df_twenty_news['text'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GKVzw2K00Eo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.to_csv('data/20N_training.csv', index=False, header=False)\n",
        "X_test.to_csv('data/20N_testing.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "Cr6d3Vmk0FzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_twenty_news = pd.read_csv('data/20N_training.csv')\n",
        "df_train_twenty_news.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaGDh19F0GjQ",
        "outputId": "9bedc7cc-c9fb-458c-f687-638fe8bf4223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(377397, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_twenty_news = pd.read_csv('data/20N_testing.csv')\n",
        "df_test_twenty_news.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlS9KiP10HaU",
        "outputId": "2edfdafe-8d14-4c02-9ccd-7723b227d380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(94349, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-Gramas"
      ],
      "metadata": {
        "id": "Zcgf1PAd0If0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funciones auxiliares"
      ],
      "metadata": {
        "id": "pFG53yAn0K-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_tokenizer(text):\n",
        "    \"\"\"\n",
        "    Tokeniza un texto personalizado (para los casos de <s>, </s> y <UNK>).\n",
        "\n",
        "    Args:\n",
        "    text: El texto a tokenizar.\n",
        "\n",
        "    Returns:\n",
        "    Una lista de tokens.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r\"<[^>]+>|[\\w']+|[.,!?;]\", text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Xt8gK-da0NPa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cálculo de probabilidades para **unigramas**, **bigramas** y **trigramas**."
      ],
      "metadata": {
        "id": "dufh0p_90N7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_unigrams_probabilities(corpus: List[str]) -> dict:\n",
        "  \"\"\"\n",
        "  Calcula las probabilidades de unigramas.\n",
        "\n",
        "  Args:\n",
        "    corpus: Lista de palabras.\n",
        "\n",
        "  Returns:\n",
        "    dict: Un diccionario con las probabilidades de unigramas.\n",
        "  \"\"\"\n",
        "  unigrams = Counter(corpus.split())\n",
        "  V = len(unigrams) # tamaño del vocabulario\n",
        "  total_words_in_corpus = sum(unigrams.values())\n",
        "\n",
        "  # Se aplica el suavizado de Laplace\n",
        "  probabilities = {}\n",
        "  for word, count in unigrams.items():\n",
        "    probabilities[word] = (count + 1) / (total_words_in_corpus + V)\n",
        "\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "P_JPXw-b0Qv4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bigrams_probabilities(corpus: List[str]) -> dict:\n",
        "  \"\"\"\n",
        "  Calcula las probabilidades de bigramas.\n",
        "\n",
        "  Args:\n",
        "    corpus: Lista de palabras.\n",
        "\n",
        "  Returns:\n",
        "    dict: Un diccionario con las probabilidades de bigramas.\n",
        "  \"\"\"\n",
        "  unigrams_count = Counter(corpus.split())\n",
        "\n",
        "  bigram_tokens = custom_tokenizer(corpus)\n",
        "  bigrams_list = list(nltk.bigrams(bigram_tokens))\n",
        "  bigrams = Counter(bigrams_list)\n",
        "  V = len(bigrams) # tamaño del vocabulario\n",
        "\n",
        "  # Se aplica el suavizado de Laplace\n",
        "  probabilities = {}\n",
        "  for bigram, count in bigrams.items():\n",
        "    probabilities[bigram] = (count + 1) / (unigrams_count[bigram[0]] + V) # [0] es el primer token del bigrama\n",
        "\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "pmfodsJz0RwK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_trigrams_probabilities(corpus: List[str]) -> dict:\n",
        "  \"\"\"\n",
        "  Calcula las probabilidades de bigramas.\n",
        "\n",
        "  Args:\n",
        "    corpus: Lista de palabras.\n",
        "\n",
        "  Returns:\n",
        "    dict: Un diccionario con las probabilidades de bigramas.\n",
        "  \"\"\"\n",
        "  tokens = custom_tokenizer(corpus)\n",
        "\n",
        "  bigrams_list = list(nltk.bigrams(tokens))\n",
        "  bigrams_count = Counter(bigrams_list)\n",
        "\n",
        "  trigrams_list = list(nltk.trigrams(tokens))\n",
        "  trigrams = Counter(trigrams_list)\n",
        "  V = len(trigrams) # tamaño del vocabulario\n",
        "\n",
        "  # Se aplica el suavizado de Laplace\n",
        "  probabilities = {}\n",
        "  for trigram, count in trigrams.items():\n",
        "    probabilities[trigram] = (count + 1) / (bigrams_count[(trigram[:2])] + V) # [:2] es el bigrama de w_i-2 y w_i-1\n",
        "\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "3afvSt4L0S0M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Probabilidades de cada modelo"
      ],
      "metadata": {
        "id": "cXZiru4r0Uf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/20N_training.csv', 'r') as f:\n",
        "  docs = f.read()"
      ],
      "metadata": {
        "id": "Yfpr3xps0acn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams_probabilities = calculate_unigrams_probabilities(docs)\n",
        "for trigram, prob in list(unigrams_probabilities.items())[:5]:\n",
        "    print(f\"{trigram}: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYcMIfa50bMG",
        "outputId": "00a439bb-c286-456f-f517-c363640c495e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>: 0.046724057927090466\n",
            "<UNK>: 0.2907921667788344\n",
            "from: 0.003012186914554917\n",
            "</s>: 0.046724057927090466\n",
            "i: 0.013219204505330321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams_probabilities = calculate_bigrams_probabilities(docs)\n",
        "for trigram, prob in list(bigrams_probabilities.items())[:5]:\n",
        "    print(f\"{trigram}: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzaAvS990cD1",
        "outputId": "a3ec9c72-ad8d-49b0-cebb-767bc234f7a1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<s>', '<UNK>'): 0.1218935563803793\n",
            "('<UNK>', 'from'): 0.003503323336405251\n",
            "('from', '<UNK>'): 0.012334273897414092\n",
            "('<UNK>', '</s>'): 0.06298874017241446\n",
            "('</s>', '<s>'): 0.43354712121577643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams_probabilities = calculate_trigrams_probabilities(docs)\n",
        "for trigram, prob in list(trigrams_probabilities.items())[:5]:\n",
        "    print(f\"{trigram}: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65jOAOUi0c_c",
        "outputId": "7c96a0e1-40c9-47fd-ea5f-6ce5bf9289be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<s>', '<UNK>', 'from'): 0.00015126854505457393\n",
            "('<UNK>', 'from', '<UNK>'): 0.0021050996473895955\n",
            "('from', '<UNK>', '</s>'): 0.0003244385687090521\n",
            "('<UNK>', '</s>', '<s>'): 0.10065140077875707\n",
            "('</s>', '<s>', 'i'): 0.015974772202682324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generar archivos con probabilidades"
      ],
      "metadata": {
        "id": "mVF2RPdi0eMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams_df = pd.DataFrame(unigrams_probabilities.items(), columns=['N-Gram', 'Probability'])\n",
        "bigrams_df = pd.DataFrame(bigrams_probabilities.items(), columns=['N-Gram', 'Probability'])\n",
        "trigrams_df = pd.DataFrame(trigrams_probabilities.items(), columns=['N-Gram', 'Probability'])"
      ],
      "metadata": {
        "id": "LmIggCZc2Xo-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams_df.to_csv('data/20N_unigrams.csv', index=False)\n",
        "bigrams_df.to_csv('data/20N_bigrams.csv', index=False)\n",
        "trigrams_df.to_csv('data/20N_trigrams.csv', index=False)"
      ],
      "metadata": {
        "id": "97BFZgvc2rM2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generación de texto"
      ],
      "metadata": {
        "id": "WxlHVmSW0h1p"
      }
    }
  ]
}