{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de los modelos de lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrantes**: Paula Daza, Nicolás Klopstock, Isabella Martínez, Gustavo Mendez. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, importamos las librerías que vamos a usar en todo el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, zipfile, random, pickle\n",
    "from typing import List, Tuple\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt')  # Tokenizador\n",
    "\n",
    "# Configuración de NLTK\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Estructuración y organización de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datasets utilizados son:\n",
    "\n",
    "- [20Newsgroups](http://qwone.com/~jason/20Newsgroups/)\n",
    "- [BAC: The Blog Authorship Corpus](https://huggingface.co/datasets/barilan/blog_authorship_corpus)\n",
    "\n",
    "Fueron descargados desde el link de Dropbox proporcionado, en el siguiente [enlace](https://huggingface.co/datasets/barilan/blog_authorship_corpus).\n",
    "\n",
    "Para el siguiente proceso vamos a suponer la siguiente estructura en el directorio.\n",
    "\n",
    "```\n",
    "data\n",
    "│\n",
    "└───20news-18828\n",
    "│   └───20news-18828\n",
    "│       └───alt.atheism\n",
    "│       └───comp.graphics\n",
    "│       └───...\n",
    "│\n",
    "└───BAC\n",
    "│   │   5114.male.25.indUnk.Scorpio.xml\n",
    "│   │   blogs.zip\n",
    "│\n",
    "└───EN_Lexicons\n",
    "│   │   AFINN-111.txt\n",
    "│   │   senticnet5.py\n",
    "│   │   SentiWordNet_3.0.0.txt\n",
    "│   │   WordStat Sentiments.txt\n",
    "│\n",
    "└───Multi Domain Sentiment\n",
    "│   │   negative.review\n",
    "│   │   processed_acl.tar.gz\n",
    "│   │   unlabeled.review\n",
    "```\n",
    "\n",
    "Nótese que el único paso adicional con respecto a los datos descargados de Dropbox es extraer el archivo `20news-18828.tar.gz`, que da lugar a la carpeta con el mismo nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir las rutas para acceder a los textos en `BAC` y `20N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_n_path = os.path.join('data', '20news-18828', '20news-18828')\n",
    "bac_path = os.path.join('data', 'BAC', 'blogs.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los textos de `20N` se encuentran en un formato que es conveniente limpiar primero con expresiones `regex`. A continuación, definimos una expresión regular para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_n_regex = r'''\n",
    "^From:.*?\\n|                      # Ignora la línea que empieza con 'From:' y lo que sigue hasta el final de la línea\n",
    "^Subject:.*?\\n|                   # Ignora la línea que empieza con 'Subject:' y lo que sigue hasta el final de la línea\n",
    "^Archive-name:.*?\\n|              # Ignora la línea que empieza con 'Archive-name:' y lo que sigue hasta el final de la línea\n",
    "^Alt-atheism-archive-name:.*?\\n|  # Ignora la línea que empieza con 'Alt-atheism-archive-name:' y lo que sigue hasta el final de la línea\n",
    "^Last-modified:.*?\\n|             # Ignora la línea que empieza con 'Last-modified:' y lo que sigue hasta el final de la línea\n",
    "^Version:.*?\\n|                   # Ignora la línea que empieza con 'Version:' y lo que sigue hasta el final de la línea\n",
    "^.*@.*?\\n|                        # Ignora la línea que contiene '@' y lo que sigue hasta el final de la línea\n",
    "In\\sarticle.*?writes:\\n|          # Ignora todo lo que está entre 'In article...' y 'writes:'\n",
    "[^a-zA-Z0-9\\s.,]                  # Elimina cualquier carácter que no sea una letra, un número o un espacio\n",
    "|^>+                              # Elimina '>' al inicio de una línea\n",
    "|\\s*>+                            # Elimina '>' seguido por espacios\n",
    "^-+$                              # Ignora las líneas que contienen solo '-'\n",
    "^=+$                              # Ignora las líneas que contienen solo '='\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda, definimos una función para listar los nombres de los directorios de una ruta en particular, y listar de la misma manera los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_directories(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retorna una lista de directorios dentro de la ruta especificada.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta en la que buscar subdirectorios.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de nombres de los subdirectorios.\n",
    "    \"\"\"\n",
    "    return [name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))]\n",
    "\n",
    "def list_files(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retorna una lista de nombres de archivos dentro de la ruta especificada.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta en la que buscar archivos.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de nombres de archivos\n",
    "    \"\"\"\n",
    "    return [name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes funciones obtienen los textos relevantes de `20N` y `BAC`. Nótese que reciben un archivo o la ruta a este, y extraen el texto que nos interesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_text_twenty_news(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae y limpia el texto relevante de un archivo utilizando regex.\n",
    "\n",
    "    Args:\n",
    "        path (str): La ruta del archivo a leer y procesar.\n",
    "\n",
    "    Returns:\n",
    "        str: El texto limpio.\n",
    "    \"\"\"\n",
    "    global twenty_n_regex # Usar una expresión regular global predefinida\n",
    "\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            # Limpiar el texto utilizando regex\n",
    "            cleaned_text = re.sub(twenty_n_regex, '', text, flags=re.VERBOSE | re.MULTILINE)\n",
    "    except (UnboundLocalError, UnicodeDecodeError) as e:\n",
    "        return ''\n",
    "\n",
    "\n",
    "    # Eliminar espacios en blanco repetidos y retornar el texto\n",
    "    return ' '.join(cleaned_text.split())\n",
    "\n",
    "def get_relevant_text_bac(content: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extrae y limpia el texto relevante de datos BAC, incluyendo fechas y el texto en cuestión (las publicaciones).\n",
    "\n",
    "    Args:\n",
    "        content (str): El contenido del texto a procesar.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: Una lista de tuplas donde cada tupla contiene una fecha y su texto.\n",
    "    \"\"\"\n",
    "    # Extraer todas las fechas y publicaciones del contenido\n",
    "    dates = re.findall(r'<date>(.*?)</date>', content, re.DOTALL)\n",
    "    posts = re.findall(r'<post>(.*?)</post>', content, re.DOTALL)\n",
    "    \n",
    "    # Limpiar los textos eliminando espacios innecesarios y palabras irrelevantes\n",
    "    cleaned_posts = [' '.join(re.sub(r'\\s+', ' ', post).split()).replace('urlLink', '') for post in posts]\n",
    "    \n",
    "    # Combinar las fechas y los textos en tuplas y retornarlas\n",
    "    return list(zip(dates, cleaned_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes son las funciones de procesamiento principales para consolidar todos los archivos tanto de `BAC` como de `20N` en sus propios `txt` aparte. Nótese que generan archivos llamados `consolidated_bac.txt` y `consolidated_news.txt` en el mismo directorio en donde se ejecute este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_twenty_news_data(path: str) -> None:\n",
    "    \"\"\"\n",
    "    Procesa todos los datos de 20N, consolida el texto y lo escribe en un archivo.\n",
    "\n",
    "    Args:\n",
    "        ruta (str): La ruta del directorio que contiene los datos de noticias.\n",
    "    \"\"\"\n",
    "    with open('consolidated_news.txt', 'w', encoding='utf-8') as output_file:\n",
    "\n",
    "        # Listar los subdirectorios que contienen las noticias\n",
    "        news_dirs = list_directories(path)\n",
    "        \n",
    "        # Para cada subdirectorio\n",
    "        for each_news_dir in news_dirs:\n",
    "            each_path = os.path.join(path, each_news_dir) # Ruta completa del subdirectorio\n",
    "            news_files = list_files(each_path) # Listar archivos dentro del subdirectorio\n",
    "            \n",
    "            # Para cada archivo de noticias\n",
    "            for each_news_file in news_files:\n",
    "                file_path = os.path.join(each_path, each_news_file) # Ruta completa del archivo\n",
    "                text = get_relevant_text_twenty_news(file_path) # Extraer el texto relevante\n",
    "                if text:\n",
    "                    output_file.write(text + '\\n') # Escribir el texto en el archivo de salida\n",
    "\n",
    "def process_bac_data(zip_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Procesa todos los datos de BAC desde un archivo zip, extrae información relevante y la escribe en un archivo.\n",
    "\n",
    "    Args:\n",
    "        ruta_zip (str): La ruta al archivo zip que contiene los datos de BAC.\n",
    "    \"\"\"\n",
    "    # Abrir el archivo zip para lectura\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_names = zip_ref.namelist() # Obtener la lista de nombres de archivos en el zip\n",
    "        \n",
    "        with open('consolidated_bac.txt', 'w', encoding='utf-8') as output_file:\n",
    "            # Para cada archivo en el zip\n",
    "            for file_name in file_names:\n",
    "                if not file_name.endswith('/'): # Ignorar directorios dentro del zip\n",
    "                    with zip_ref.open(file_name) as file:\n",
    "                        try:\n",
    "                            # Intentar leer el contenido del archivo como UTF-8\n",
    "                            content = file.read().decode('utf-8')\n",
    "                        except UnicodeDecodeError:\n",
    "                            # Si falla, leerlo como latin1\n",
    "                            content = file.read().decode('latin1')\n",
    "                        \n",
    "                        # Extraer las parejas de fecha y texto\n",
    "                        data_pairs = get_relevant_text_bac(content)\n",
    "                        \n",
    "                        # Escribir cada pareja en el archivo de salida\n",
    "                        for date, post in data_pairs:\n",
    "                            output_file.write(f\"{date} - {post}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, la siguiente celda ejecuta estos procesos y genera los archivos consolidados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_twenty_news_data(twenty_n_path)\n",
    "process_bac_data(bac_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Preprocesamiento de texto (tokenización, tokens especiales, normalización)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Procesa el texto de la ruta especificada, aplicando lo siguiente:\n",
    "\n",
    "    - Tokenización por oración\n",
    "    - Normalización\n",
    "    - Reemplaza números por NUM\n",
    "    - Añade tokens de inicio y finalización de oración <s> y </s>\n",
    "    - Tokens con frecuencia unitaria se marcan como <UNK>\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta en la que leer el archivo de texto.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Listado de oraciones preprocesadas.\n",
    "    \"\"\"\n",
    "    # Leer el archivo de texto línea por línea\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sentences = []\n",
    "    for line in lines:\n",
    "        # Dividir cada línea en oraciones\n",
    "        line_sentences = sent_tokenize(line.strip())\n",
    "        sentences.extend(line_sentences)\n",
    "    \n",
    "    # Listas donde guardar tokens y oraciones procesadas\n",
    "    all_tokens = []\n",
    "    processed_sentences = []\n",
    "    \n",
    "    # Para cada oración...\n",
    "    for sentence in tqdm(sentences, desc=\"Procesando oraciones...\"):\n",
    "\n",
    "        # Pasar a minúsculas\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Separar números de letras (e.g. 9am -> 9 am)\n",
    "        sentence = re.sub(r'(\\d+)([a-zA-Z])', r'\\1 \\2', sentence)\n",
    "        sentence = re.sub(r'([a-zA-Z])(\\d+)', r'\\1 \\2', sentence)\n",
    "        \n",
    "        # Reemplazar números por el token NUM\n",
    "        sentence = re.sub(r'\\d+', 'NUM', sentence)\n",
    "        \n",
    "        # Eliminar puntuación y caracteres especiales\n",
    "        sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "        \n",
    "        # Tokenizar palabras con nltk\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Añadir tokens de inicio y finalización\n",
    "        tokens = ['<s>'] + tokens + ['</s>']\n",
    "        \n",
    "        # Agregar a las listas predefinidas si no es una oración vacía\n",
    "        if len(tokens) > 2:\n",
    "            all_tokens.extend(tokens)\n",
    "            processed_sentences.append(tokens)\n",
    "    \n",
    "    # Contar la frecuencia de los tokens\n",
    "    token_freq = Counter(all_tokens)\n",
    "    \n",
    "    # Reemplazar tokens de frecuencia unitaria con <UNK>\n",
    "    final_sentences = []\n",
    "    for tokens in processed_sentences:\n",
    "        final_tokens = [token if token_freq[token] > 1 else '<UNK>' for token in tokens]\n",
    "        final_sentences.append(final_tokens)\n",
    "    \n",
    "    return final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando oraciones...: 100%|██████████| 240621/240621 [00:17<00:00, 13849.94it/s]\n"
     ]
    }
   ],
   "source": [
    "final_sentences_news = preprocess_text('consolidated_news.txt')\n",
    "with open('processed_news.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for sentence in final_sentences_news:\n",
    "        output_file.write(' '.join(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando oraciones...: 100%|██████████| 7144331/7144331 [07:47<00:00, 15266.19it/s]\n"
     ]
    }
   ],
   "source": [
    "final_sentences_bac = preprocess_text('consolidated_bac.txt')\n",
    "with open('processed_bac.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for sentence in final_sentences_bac:\n",
    "        output_file.write(' '.join(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Creación de conjuntos de entrenamiento y evaluación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_testing(final_sentences: List[str], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Genera archivos de texto que contienen conjuntos de oraciones para entrenamiento y prueba, en una\n",
    "    proporción de 80-20.\n",
    "\n",
    "    Args:\n",
    "        final_sentences (List[str]): Lista de oraciones que se usarán para generar los conjuntos.\n",
    "        filename (str): Nombre base del archivo de salida, sin extensión. \n",
    "                        Se crearán dos archivos: `<filename>_training.txt` y `<filename>_testing.txt`.\n",
    "    \"\"\"\n",
    "    # Hacer una copia de las oraciones y mezclarlas aleatoriamente\n",
    "    sents = final_sentences.copy()\n",
    "    random.shuffle(sents)\n",
    "\n",
    "    # Determinar el punto de separación para 80-20\n",
    "    split_index = int(0.8 * len(sents))\n",
    "\n",
    "    # Separar en conjuntos de entrenamiento y prueba\n",
    "    training_sentences = sents[:split_index]\n",
    "    testing_sentences = sents[split_index:]\n",
    "\n",
    "    # Guardar las oraciones de entrenamiento en un archivo txt\n",
    "    with open(f'{filename}_training.txt', 'w', encoding='utf-8') as training_file:\n",
    "        for sentence in training_sentences:\n",
    "            training_file.write(' '.join(sentence) + '\\n')\n",
    "\n",
    "    # Guardar las oraciones de prueba en un archivo txt\n",
    "    with open(f'{filename}_testing.txt', 'w', encoding='utf-8') as testing_file:\n",
    "        for sentence in testing_sentences:\n",
    "            testing_file.write(' '.join(sentence) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_training_testing(final_sentences_news, \"20N\")\n",
    "generate_training_testing(final_sentences_bac, \"BAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Creación de los modelos de lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto es buena idea reiniciar el kernel y liberar memoria RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt') \n",
    "\n",
    "# Configuración de NLTK\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(file_path: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Lee un archivo de texto, tokeniza las oraciones y retorna una lista de listas de tokens.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Ruta al archivo de texto.\n",
    "        \n",
    "    Returns:\n",
    "        List[List[str]]: Lista de listas de tokens (cada oración tokenizada).\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sentences = text.strip().split('\\n')\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def build_ngram_counts(tokenized_sentences: List[List[str]], n: int) -> Counter:\n",
    "    \"\"\"\n",
    "    Cuenta los n-gramas de las oraciones tokenizadas usando nltk\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: Lista de listas de tokens.\n",
    "        n: Tamaño del n-grama.\n",
    "        \n",
    "    Returns:\n",
    "        Counter: Un diccionario con los conteos de n-gramas.\n",
    "    \"\"\"\n",
    "    ngram_counts = Counter()\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        ngrams = list(nltk.ngrams(sentence, n))\n",
    "        ngram_counts.update(ngrams)\n",
    "    \n",
    "    return ngram_counts\n",
    "\n",
    "\n",
    "def calculate_ngram_probabilities(tokenized_sentences: List[List[str]], n: int) -> Dict[Tuple[str, ...], float]:\n",
    "    \"\"\"\n",
    "    Calcula las probabilidades de n-gramas con suavizado de Laplace.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: Lista de listas de tokens.\n",
    "        n: Tamaño del n-grama.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[Tuple[str, ...], float]: Diccionario con las probabilidades de los n-gramas.\n",
    "    \"\"\"\n",
    "    # Cuenta los n-gramas y de ser necesario los (n-1)-gramas, para bigramas y trigramas.\n",
    "    ngram_counts = build_ngram_counts(tokenized_sentences, n)\n",
    "    n_minus_1_gram_counts = build_ngram_counts(tokenized_sentences, n-1) if n > 1 else None\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    vocabulary = set(token for sentence in tokenized_sentences for token in sentence)\n",
    "    vocab_size = len(vocabulary)\n",
    "\n",
    "    # Diccionario que representará nuestro modelo\n",
    "    ngram_probabilities = {}\n",
    "\n",
    "    for ngram, count in tqdm(ngram_counts.items(), desc=f\"Calculando {n}-gramas\"):\n",
    "        if n == 1:\n",
    "            # Para unigramas el conteo es sencillo\n",
    "            total_count = sum(ngram_counts.values())\n",
    "            ngram_probabilities[ngram] = (count + 1) / (total_count + vocab_size)\n",
    "        else:\n",
    "            # Para bigramas y trigramas hay que tener en cuenta el contexto\n",
    "            priori = ngram[:-1]\n",
    "            priori_count = n_minus_1_gram_counts[priori]\n",
    "            ngram_probabilities[ngram] = (count + 1) / (priori_count + vocab_size)\n",
    "    \n",
    "    return ngram_probabilities\n",
    "\n",
    "def save_ngram_model(ngram_probabilities: Dict[Tuple[str, ...], float], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda el modelo de lenguaje de n-gramas en un archivo binario usando pickle.\n",
    "    \n",
    "    Args:\n",
    "        ngram_probabilities (Dict[Tuple[str, ...], float]): Diccionario con las probabilidades de los n-gramas.\n",
    "        filename (str): Nombre del archivo donde se guardará el modelo.\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(ngram_probabilities, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando 1-gramas: 100%|██████████| 60025/60025 [00:14<00:00, 4167.51it/s]\n",
      "Calculando 2-gramas: 100%|██████████| 878070/878070 [00:00<00:00, 1326933.21it/s]\n",
      "Calculando 3-gramas: 100%|██████████| 1991606/1991606 [00:01<00:00, 1045402.69it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = read_and_tokenize('20N_training.txt')\n",
    "\n",
    "unigram_probs = calculate_ngram_probabilities(tokenized_sentences, 1)\n",
    "bigram_probs = calculate_ngram_probabilities(tokenized_sentences, 2)\n",
    "trigram_probs = calculate_ngram_probabilities(tokenized_sentences, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams:\n",
      "('<s>',): 0.045005262235729766\n",
      "('ive',): 0.0006062831306847886\n",
      "('seen',): 0.00031693680577221157\n",
      "('him',): 0.000640712277740401\n",
      "('play',): 0.00022638343269443684\n",
      "\n",
      "Bigrams:\n",
      "('<s>', 'ive'): 0.0037389436089176596\n",
      "('ive', 'seen'): 0.004984423676012461\n",
      "('seen', 'him'): 0.00019554165037152912\n",
      "('him', 'play'): 0.00022313957380341404\n",
      "('play', 'for'): 0.0005083300537845993\n",
      "\n",
      "Trigrams:\n",
      "('<s>', 'ive', 'seen'): 0.001345100226370526\n",
      "('ive', 'seen', 'him'): 3.314770617873243e-05\n",
      "('seen', 'him', 'play'): 6.66266906522753e-05\n",
      "('him', 'play', 'for'): 3.331223558413005e-05\n",
      "('play', 'for', 'the'): 0.00026642244609108317\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigrams:\")\n",
    "for unigram, prob in list(unigram_probs.items())[:5]:\n",
    "    print(f\"{unigram}: {prob}\")\n",
    "\n",
    "print(\"\\nBigrams:\")\n",
    "for bigram, prob in list(bigram_probs.items())[:5]:\n",
    "    print(f\"{bigram}: {prob}\")\n",
    "\n",
    "print(\"\\nTrigrams:\")\n",
    "for trigram, prob in list(trigram_probs.items())[:5]:\n",
    "    print(f\"{trigram}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ngram_model(unigram_probs, \"20N_unigrams.pkl\")\n",
    "save_ngram_model(bigram_probs, \"20N_bigrams.pkl\")\n",
    "save_ngram_model(trigram_probs, \"20N_trigrams.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando 1-gramas: 100%|██████████| 270847/270847 [04:56<00:00, 914.27it/s]\n",
      "Calculando 2-gramas: 100%|██████████| 8902311/8902311 [00:06<00:00, 1286333.39it/s]\n",
      "Calculando 3-gramas: 100%|██████████| 31428880/31428880 [00:31<00:00, 1003459.65it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences_bac = read_and_tokenize('BAC_training.txt')\n",
    "\n",
    "unigram_probs_bac = calculate_ngram_probabilities(tokenized_sentences_bac, 1)\n",
    "bigram_probs_bac = calculate_ngram_probabilities(tokenized_sentences_bac, 2)\n",
    "trigram_probs_bac = calculate_ngram_probabilities(tokenized_sentences_bac, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams:\n",
      "('<s>',): 0.05508172833221783\n",
      "('i',): 0.0338773017666548\n",
      "('ll',): 0.0012488989901693823\n",
      "('kill',): 0.00011111598484642159\n",
      "('you',): 0.008370967397909566\n",
      "\n",
      "Bigrams:\n",
      "('<s>', 'i'): 0.15295956856258663\n",
      "('i', 'll'): 0.020992896480166015\n",
      "('ll', 'kill'): 0.00053326390053752\n",
      "('kill', 'you'): 0.0028410902418114963\n",
      "('you', 'when'): 0.0013790266120353178\n",
      "\n",
      "Trigrams:\n",
      "('<s>', 'i', 'll'): 0.015410658445388373\n",
      "('i', 'll', 'kill'): 0.0003745511105011551\n",
      "('ll', 'kill', 'you'): 0.0002545571259393711\n",
      "('kill', 'you', 'when'): 1.1043703616444811e-05\n",
      "('you', 'when', 'you'): 0.0024852427241490705\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigrams:\")\n",
    "for unigram, prob in list(unigram_probs_bac.items())[:5]:\n",
    "    print(f\"{unigram}: {prob}\")\n",
    "\n",
    "print(\"\\nBigrams:\")\n",
    "for bigram, prob in list(bigram_probs_bac.items())[:5]:\n",
    "    print(f\"{bigram}: {prob}\")\n",
    "\n",
    "print(\"\\nTrigrams:\")\n",
    "for trigram, prob in list(trigram_probs_bac.items())[:5]:\n",
    "    print(f\"{trigram}: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ngram_model(unigram_probs_bac, \"BAC_unigrams.pkl\")\n",
    "save_ngram_model(bigram_probs_bac, \"BAC_bigrams.pkl\")\n",
    "save_ngram_model(trigram_probs_bac, \"BAC_trigrams.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
