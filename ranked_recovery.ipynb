{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperación ranqueada y vectorización de documentos (RRDV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos librerías de utilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt')  # Tokenizador\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "\n",
    "# Configuración de NLTK\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos nuestras funciones de preprocesamiento de tokens y texto. Usamos la librería de `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def token_preprocessing(tokens: List[str]) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Realiza el preprocesamiento de texto, aplicando las siguientes transformaciones:\n",
    "#     1. Convierte todos los tokens a minúsculas.\n",
    "#     2. Elimina stopwords.\n",
    "#     3. Elimina tokens que no son palabras (puntuación, números, etc.).\n",
    "#     4. Aplica stemming con PorterStemmer.\n",
    "\n",
    "#     Args:\n",
    "#         tokens (List[str]): Lista de tokens a preprocesar.\n",
    "\n",
    "#     Returns:\n",
    "#         List[str]: Lista de tokens preprocesados.\n",
    "#     \"\"\"\n",
    "#     # Pasa los tokens a minúsculas\n",
    "#     tokens = [word.lower() for word in tokens]\n",
    "\n",
    "#     # Elimina stopwords (asume texto en inglés)\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "#     # Filtrar tokens no alfabéticos (palabras que no empiezan con una letra)\n",
    "#     tokens = [word for word in tokens if 'a' <= word[0] <= 'z']\n",
    "\n",
    "#     # Aplica stemming\n",
    "#     ps = PorterStemmer()\n",
    "#     return [ps.stem(word) for word in tokens]\n",
    "\n",
    "def token_preprocessing(tokens: List[str]) -> List[str]:\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Realiza el preprocesamiento de texto, aplicando las siguientes transformaciones:\n",
    "    1. Convierte todos los tokens a minúsculas.\n",
    "    2. Elimina stopwords.\n",
    "    3. Elimina tokens que no son palabras (puntuación, números, etc.).\n",
    "    4. Aplica stemming con PorterStemmer.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): Lista de tokens a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens preprocesados.\n",
    "    \"\"\"\n",
    "    # Pasa los tokens a minúsculas\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Elimina stopwords (asume texto en inglés)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [re.sub(r'\\W', '', word) for word in tokens]\n",
    "    tokens = [re.sub(r'\\s+[a-zA-Z]\\s+', '', word) for word in tokens]\n",
    "    tokens = [re.sub(r'\\^[a-zA-Z]\\s+', '', word) for word in tokens]\n",
    "    tokens = [re.sub(r'[0-9]+', '', word) for word in tokens]\n",
    "\n",
    "    tokens = [word for word in tokens if len(word) > 0]\n",
    "\n",
    "    # Aplica stemming\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in tokens]\n",
    "\n",
    "def text_preprocessing(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocesa un texto mediante la tokenización y el preprocesamiento de tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens preprocesados.\n",
    "    \"\"\"\n",
    "    # Tokenizar el texto usando el tokenizador de nltk\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Preprocesar los tokens\n",
    "    return token_preprocessing(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una clase `Document`. Esta nos servirá para almacenar cada uno de los documentos en una estructura que nos permita acceder rápidamente al conteo de sus tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, text: str, name: str = 'nameless'):\n",
    "        \"\"\"\n",
    "        Crea un objeto del tipo Document.\n",
    "\n",
    "        Args:\n",
    "            text (str): El contenido del documento.\n",
    "            name (str): El nombre del documento. Por defecto es 'nameless'.\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.name = name\n",
    "\n",
    "        # Preprocesar el texto y contar la frecuencia de cada token\n",
    "        counter = Counter(text_preprocessing(text))\n",
    "        \n",
    "        # Almacenar los conteos de términos como una Serie de pandas\n",
    "        self.term_counts = pd.Series(counter.values(), index=counter.keys())\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Representación formal del objeto Document.\n",
    "        \"\"\"\n",
    "        return str(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Representación informal del objeto Document, devuelve el nombre del documento.\n",
    "        \"\"\"\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función se encarga de cargar los documentos, parsearlos y convertirlos en objetos de tipo `Document`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(docs_folder_path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Carga los documentos (con extensión .naf) en una carpeta especificada y crea objetos Document para cada uno.\n",
    "\n",
    "    Args:\n",
    "        docs_folder_path (Path): Ruta a la carpeta que contiene los archivos .naf.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: Una lista de objetos Document.\n",
    "    \"\"\"\n",
    "    # Encuentra todos los archivos con extensión .naf en la carpeta especificada\n",
    "    files = [f for f in os.listdir(docs_folder_path) if f.endswith('.naf')]\n",
    "\n",
    "    # Lista para almacenar los documentos cargados\n",
    "    docs = []\n",
    "\n",
    "    # Procesar cada archivo .naf\n",
    "    for file in files:\n",
    "        # Parsea el archivo XML\n",
    "        tree = ET.parse(os.path.join(docs_folder_path, file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extrae el texto crudo del documento\n",
    "        raw_text = root.find('.//raw').text\n",
    "\n",
    "        # Extrae el nombre del archivo, omitiendo la extensión\n",
    "        name = file.split('.')[1]\n",
    "\n",
    "        # Crea un objeto Document y lo añade a la lista\n",
    "        docs.append(Document(raw_text, name))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = load_docs(Path(\"./data/docs-raw-texts\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la clase para la **Recuperación Ranqueada y Vectorización de Documentos.** Como parte de sus atributos almacena el conteo de términos por documento, y su IDF y TF-IDF. Con estos se hará la búsqueda ranqueada, haciendo uso de la similitud coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRDV:\n",
    "    def __init__(self, docs: List[Document]):\n",
    "        \"\"\"\n",
    "        Inicializa RRDV con una lista de documentos.\n",
    "\n",
    "        Args:\n",
    "            docs (List[Document]): Lista de objetos Document para construir el índice TF-IDF.\n",
    "        \"\"\"\n",
    "        self.docs = docs\n",
    "        \n",
    "        # Crear un DataFrame con los conteos de términos para cada documento\n",
    "        self.term_counts = pd.DataFrame({\n",
    "            doc.name: doc.term_counts for doc in self.docs\n",
    "        })\n",
    "        self.term_counts.fillna(0, inplace=True)\n",
    "\n",
    "        # Calcular la frecuencia de documentos para cada término\n",
    "        self.document_count = (self.term_counts >= 1).sum(axis=1)\n",
    "\n",
    "        # Calcular el IDF (Inverse Document Frequency)\n",
    "        self.idf = np.log10(len(self.docs) / self.document_count)\n",
    "\n",
    "        # Calcular TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "        self.tfidf = np.log10(1 + self.term_counts).mul(self.idf, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_similarity(tfidf_doc_1: pd.Series, tfidf_doc_2: pd.Series | pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calcula la similitud coseno entre dos vectores.\n",
    "\n",
    "        Args:\n",
    "            tfidf_doc_1 (pd.Series): Vector del primer documento.\n",
    "            tfidf_doc_2 (pd.Series | pd.DataFrame): Vector TF-IDF del segundo documento o un DataFrame de vectores.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: Similitudes coseno entre tfidf_doc_1 y tfidf_doc_2.\n",
    "        \"\"\"\n",
    "        return np.dot(tfidf_doc_1, tfidf_doc_2) / (\n",
    "            np.linalg.norm(tfidf_doc_1) * np.linalg.norm(tfidf_doc_2, axis=0))\n",
    "\n",
    "    def search(self, query_document: Document, min_similarity: float = 0.0) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Realiza una búsqueda para encontrar documentos similares al documento de consulta.\n",
    "\n",
    "        Args:\n",
    "            query_document (Document): Documento de consulta para buscar similitudes.\n",
    "            min_similarity (float): Umbral mínimo de similitud para filtrar resultados. Por defecto es 0.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame con documentos relevantes y sus similitudes.\n",
    "        \"\"\"\n",
    "        # Filtrar términos en el vocabulario del índice\n",
    "        in_vocab_term_counts = query_document.term_counts[query_document.term_counts.index.isin(self.idf.index)]\n",
    "        \n",
    "        # Calcular el TF-IDF para el documento de consulta\n",
    "        query_tfidf = (np.log10(1 + in_vocab_term_counts) * self.idf).fillna(0)\n",
    "\n",
    "        # Calcular la similitud coseno entre el documento de consulta y todos los documentos en el índice\n",
    "        similarities = self.cosine_similarity(query_tfidf, self.tfidf)\n",
    "        \n",
    "        # Crear un DataFrame con los resultados de similitud\n",
    "        results = pd.DataFrame({\n",
    "            'similarity': similarities,\n",
    "            'doc': self.docs\n",
    "        }, index=self.tfidf.columns)\n",
    "        \n",
    "        # Ordenar los resultados por similitud de mayor a menor\n",
    "        results.sort_values(by='similarity', ascending=False, inplace=True)\n",
    "\n",
    "        # Filtrar los resultados por similitud mínima\n",
    "        results = results[results['similarity'] > min_similarity]\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def evaluate_search(self, queries: List[Document], output_path: Path):\n",
    "        \"\"\"\n",
    "        Evalúa las consultas y escribe los resultados en un archivo de salida.\n",
    "\n",
    "        Args:\n",
    "            queries (List[Document]): Lista de documentos de consulta para evaluar.\n",
    "            output_path (Path): Ruta del archivo donde se guardarán los resultados.\n",
    "        \"\"\"\n",
    "        with open(output_path, 'w') as output_file:\n",
    "            for query in queries:\n",
    "                relevant_docs = self.search(query_document=query)    \n",
    "                result_texts = [f'{doc_name}:{row.similarity}' for doc_name, row in relevant_docs.iterrows()]\n",
    "                output_file.write(f\"{query.name}\\t{','.join(result_texts)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrdv = RRDV(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = load_docs(Path(\"./data/queries-raw-texts\"))\n",
    "rrdv.evaluate_search(all_queries, output_path=Path(\"./data/RRDV-consultas_resultado\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
