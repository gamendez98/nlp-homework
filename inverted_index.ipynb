{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda binaria con índice invertido (BSII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos librerías de utilidad. Entre ellas destaca `nltk`, que será la principal encargada de la limpieza y el preprocesamiento del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt')  # Tokenizador\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "\n",
    "# Configuración de NLTK\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_preprocessing(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Realiza el preprocesamiento de texto, aplicando las siguientes transformaciones:\n",
    "    1. Convierte todos los tokens a minúsculas.\n",
    "    2. Elimina stopwords.\n",
    "    3. Elimina tokens que no son palabras (puntuación, números, etc.).\n",
    "    4. Aplica stemming con PorterStemmer.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[str]): Lista de tokens a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens preprocesados.\n",
    "    \"\"\"\n",
    "    # Pasa los tokens a minúsculas\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    # Elimina stopwords (asume texto en inglés)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Filtrar tokens no alfabéticos (palabras que no empiezan con una letra)\n",
    "    tokens = [word for word in tokens if 'a' <= word[0] <= 'z']\n",
    "\n",
    "    # Aplica stemming\n",
    "    ps = PorterStemmer()\n",
    "    return [ps.stem(word) for word in tokens]\n",
    "\n",
    "def text_preprocessing(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocesa un texto mediante la tokenización y el preprocesamiento de tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de tokens preprocesados.\n",
    "    \"\"\"\n",
    "    # Tokenizar el texto usando el tokenizador de nltk\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Preprocesar los tokens\n",
    "    return token_preprocessing(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['william',\n",
       " 'beaumont',\n",
       " 'physiolog',\n",
       " 'digest',\n",
       " 'imag',\n",
       " 'sourc',\n",
       " 'novemb',\n",
       " 'us-american',\n",
       " 'surgeon',\n",
       " 'william',\n",
       " 'beaumont',\n",
       " 'born']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing(\n",
    "    \"William Beaumont: Physiology of digestion Image Source.  On November 21, 1785, US-American surgeon William Beaumont was born.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, text: str, name: str = 'nameless'):\n",
    "        \"\"\"\n",
    "        Crea un objeto del tipo Document.\n",
    "\n",
    "        Args:\n",
    "            text (str): El contenido del documento.\n",
    "            name (str): El nombre del documento. Por defecto es 'nameless'.\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.name = name\n",
    "\n",
    "        # Preprocesar el texto y contar la frecuencia de cada token\n",
    "        counter = Counter(text_preprocessing(text))\n",
    "        \n",
    "        # Almacenar los conteos de términos como una Serie de pandas\n",
    "        self.term_counts = pd.Series(counter.values(), index=counter.keys())\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Representación formal del objeto Document.\n",
    "        \"\"\"\n",
    "        return str(self)\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Representación informal del objeto Document, devuelve el nombre del documento.\n",
    "        \"\"\"\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(docs_folder_path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Carga los documentos (con extensión .naf) en una carpeta especificada y crea objetos Document para cada uno.\n",
    "\n",
    "    Args:\n",
    "        docs_folder_path (Path): Ruta a la carpeta que contiene los archivos .naf.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: Una lista de objetos Document.\n",
    "    \"\"\"\n",
    "    # Encuentra todos los archivos con extensión .naf en la carpeta especificada\n",
    "    files = [f for f in os.listdir(docs_folder_path) if f.endswith('.naf')]\n",
    "\n",
    "    # Lista para almacenar los documentos cargados\n",
    "    docs = []\n",
    "\n",
    "    # Procesar cada archivo .naf\n",
    "    for file in files:\n",
    "        # Parsea el archivo XML\n",
    "        tree = ET.parse(os.path.join(docs_folder_path, file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extrae el texto crudo del documento\n",
    "        raw_text = root.find('.//raw').text\n",
    "\n",
    "        # Extrae el nombre del archivo, omitiendo la extensión\n",
    "        name = file.split('.')[1]\n",
    "\n",
    "        # Crea un objeto Document y lo añade a la lista\n",
    "        docs.append(Document(raw_text, name))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = load_docs(Path(\"./data/docs-raw-texts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSII:\n",
    "    def __init__(self, docs: List[Document]):\n",
    "        \"\"\"\n",
    "        Inicializa el índice invertido utilizando una lista de documentos.\n",
    "\n",
    "        Args:\n",
    "            docs (List[Document]): Lista de objetos Document a indexar.\n",
    "        \"\"\"\n",
    "        self.docs = docs\n",
    "        self.inverse_index = {}\n",
    "\n",
    "        # Construir el índice invertido\n",
    "        for doc in self.docs:\n",
    "\n",
    "            # Procesa cada término del documento\n",
    "            for term in doc.term_counts.index:\n",
    "\n",
    "                # Si el término no está en el índice invertido, lo agrega\n",
    "                if term not in self.inverse_index:\n",
    "                    self.inverse_index[term] = set()\n",
    "\n",
    "                # Añade el documento al listing del término\n",
    "                self.inverse_index[term].add(doc)\n",
    "\n",
    "    def search(self, query_document: Document = None, excluded_query_document: Document = None) -> set:\n",
    "        \"\"\"\n",
    "        Realiza una búsqueda en el índice invertido, incluyendo o excluyendo documentos según los términos. Note que procesamos los queries como objetos de tipo Documento.\n",
    "\n",
    "        Args:\n",
    "            query_document (Document, opcional): Documento cuyas palabras clave se utilizarán para buscar documentos relevantes. Esto implementa el AND.\n",
    "            excluded_query_document (Document, opcional): Documento cuyas palabras clave se utilizarán para excluir documentos de los resultados. Esto es equivalente a un NOT.\n",
    "\n",
    "        Returns:\n",
    "            set: Un conjunto de documentos que cumplen con los criterios de la búsqueda.\n",
    "        \"\"\"\n",
    "        relevant_docs = set(self.docs)\n",
    "\n",
    "        # Incluir documentos relevantes según el query_document\n",
    "        if query_document is not None:\n",
    "            for term in query_document.term_counts.index:\n",
    "                if term in self.inverse_index:\n",
    "                    relevant_docs.intersection_update(self.inverse_index[term])\n",
    "\n",
    "        # Excluir documentos según el excluded_query_document\n",
    "        if excluded_query_document is not None:\n",
    "            for term in excluded_query_document.term_counts.index:\n",
    "                if term in self.inverse_index:\n",
    "                    relevant_docs.difference_update(self.inverse_index[term])\n",
    "\n",
    "        return relevant_docs\n",
    "\n",
    "    def evaluate_search(self, queries: List[Document], output_path: Path):\n",
    "        \"\"\"\n",
    "        Evalúa las consultas de búsqueda y escribe los resultados en un archivo de salida.\n",
    "\n",
    "        Args:\n",
    "            queries (List[Document]): Lista de documentos a utilizar como consultas.\n",
    "            output_path (Path): Ruta del archivo donde se guardarán los resultados.\n",
    "        \"\"\"\n",
    "        with open(output_path, 'w') as output_file:\n",
    "            for query in queries:\n",
    "                relevant_docs = self.search(query_document=query)\n",
    "                # Escribir el nombre de la consulta seguido de los nombres de los documentos relevantes\n",
    "                output_file.write(f\"{query.name}\\t{','.join(doc.name for doc in relevant_docs)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{d001, d046, d062, d120, d133, d191, d261, d286, d294, d314}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsii = BSII(all_docs)\n",
    "bsii.search(query_document=Document('Physiology'), excluded_query_document=Document('Swiss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué hacemos con términos que no están en el índice invertido para las búsquedas? Por defecto ahora los está ignorando simplemente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{d001, d046, d062, d120, d133, d191, d261, d286, d294, d314}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsii.search(query_document=Document('Physiology Esternocleidomastoideo'), excluded_query_document=Document('Swiss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = load_docs(Path(\"./data/queries-raw-texts\"))\n",
    "bsii.evaluate_search(all_queries, 'data/BSII-AND-queries_result')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
